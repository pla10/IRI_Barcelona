{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the training of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import skimage.measure\n",
    "from datetime import datetime\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = 64\n",
    "step = int(32/4)\n",
    "\n",
    "map_list = ['stanford_coupa0', 'stanford_coupa1', 'stanford_coupa2', 'stanford_coupa3', 'stanford_deathCircle0', 'stanford_gates2', 'stanford_hyang2', 'stanford_hyang3', 'stanford_hyang4', 'stanford_hyang10', 'stanford_little3', 'stanford_nexus0', 'stanford_nexus1']\n",
    "val_list = [map_list.pop(0)]\n",
    "\n",
    "sem_dict = ['cash', 'entrance', 'light', 'sit', 'stairs', 'trash', 'tree','restricted','grass','intersection','shadow']\n",
    "chans = len(sem_dict)+1\n",
    "\n",
    "lut_in = [0, 20, 50, 100, 150, 255]\n",
    "lut_out = [0, 100, 180, 220, 240, 255]\n",
    "lut_8u = np.interp(np.arange(0, 256), lut_in, lut_out).astype(np.uint8)\n",
    "\n",
    "spec = 'stan'\n",
    "train_data_dir = 'training_data/64crop_size/1red/'\n",
    "\n",
    "now = datetime.now() # current date and time\n",
    "date = now.strftime(\"%d\")+now.strftime(\"%b\").lower()\n",
    "print(date)\n",
    "\n",
    "# filename = 'IRI_models/'+date+'_'+str(step)+'px_steps_'+spec+'_paths'\n",
    "# filename1 = 'IRI_models/'+date+'_'+str(step)+'px_steps_'+spec+'_vels'\n",
    "# filename2 = 'IRI_models/'+date+'_'+str(step)+'px_steps_'+spec+'_stops'\n",
    "filename = 'IRI_models/final'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "\n",
    "    \n",
    "    # CNN-11  |  CNN-21  |  CNN-31\n",
    "    fil_array = [4,8,8]                         #Num filters first conv: 4, 8 or 8\n",
    "    lay_array = [1,3,5]                         #Layers per dense block: 1, 3 or 5\n",
    "    learn_array = [8.71e-5,3.72e-4,1.51e-4]     #Learning rates\n",
    "    decaylearn_array = [0.9984,0.9984,0.9985]   #Learning rate decays\n",
    "    wdecay_array = [1.11e-6,5.53e-7,4.58e-5]    #Weight decays\n",
    "    dropout_array = [0.307,0.120,0.349]         #Dropout probability\n",
    "\n",
    "    arc = 2                                     #[0,1,2]\n",
    "\n",
    "    filters = fil_array[arc]\n",
    "    layers_in_dense = lay_array[arc]\n",
    "\n",
    "    tf.random.set_seed(0)\n",
    "    dropout_array[arc] = dropout_array[arc] * 0.01\n",
    "\n",
    "    def dense_factor(inputs):\n",
    "        h_1 = layers.BatchNormalization()(inputs)\n",
    "        output = layers.Conv2D(32, (3,3), padding='same', activation='relu')(h_1)\n",
    "        return output\n",
    "\n",
    "    def dense_block(inputs, upsampling):\n",
    "        concatenated_inputs = inputs\n",
    "        concatenated_inputs_less = []\n",
    "        for i in range(layers_in_dense):\n",
    "            x = dense_factor(concatenated_inputs)\n",
    "            concatenated_inputs = layers.concatenate([concatenated_inputs, x], axis=3)\n",
    "            concatenated_inputs_less = layers.concatenate([concatenated_inputs, x], axis=3)\n",
    "            if(i == layers_in_dense - 1 and upsampling):\n",
    "                concatenated_inputs = concatenated_inputs_less\n",
    "\n",
    "        return concatenated_inputs\n",
    "\n",
    "    norm_layer = layers.Normalization(axis=None)\n",
    "    norm_layer.adapt(train_x)\n",
    "\n",
    "    input_img = layers.Input(shape=(div, div, chans))\n",
    "\n",
    "    augment_input = norm_layer(input_img)\n",
    "    augment_input = layers.RandomRotation(factor=0.5)(augment_input)\n",
    "    augment_input = layers.RandomFlip(mode='horizontal_and_vertical')(augment_input)\n",
    "    augment_input = layers.RandomZoom(height_factor=0.2, width_factor=0.2)(augment_input)\n",
    "\n",
    "    lays = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(augment_input)\n",
    "    lays = layers.Dropout(dropout_array[arc])(lays)\n",
    "    filters = filters+2\n",
    "    lays = dense_block(lays,False)\n",
    "    lays = layers.Dropout(dropout_array[arc])(lays)\n",
    "    layerX = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(lays)\n",
    "    lays = layers.Dropout(dropout_array[arc])(lays)\n",
    "    filters = filters+1\n",
    "    lays = layers.MaxPooling2D((2, 2), strides=2)(layerX)\n",
    "    lays = dense_block(lays,False)\n",
    "    lays = layers.Dropout(dropout_array[arc])(lays)\n",
    "    layerY = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(lays)\n",
    "    lays = layers.Dropout(dropout_array[arc])(lays)\n",
    "    lays = layers.MaxPooling2D((2, 2), strides=2)(layerY)\n",
    "\n",
    "    lays = dense_block(lays,True)\n",
    "\n",
    "    lays = layers.Conv2DTranspose(filters, (3, 3), strides=2, padding='same')(lays)\n",
    "    lays = layers.Dropout(dropout_array[arc])(lays)\n",
    "    lays = layers.Add()([layerY,lays])\n",
    "    lays = dense_block(lays,True)\n",
    "    lays = layers.Dropout(dropout_array[arc])(lays)\n",
    "    filters = filters-1\n",
    "    lays = layers.Conv2DTranspose(filters, (3, 3), strides=2, padding='same')(lays)\n",
    "    lays = layers.Dropout(dropout_array[arc])(lays)\n",
    "    lays = layers.Add()([layerX,lays])\n",
    "    lays = dense_block(lays,False)\n",
    "    lays = layers.Dropout(dropout_array[arc])(lays)\n",
    "    filters = filters-2\n",
    "    lays = layers.Conv2D(filters, (1, 1), padding='same', activation='relu')(lays)\n",
    "    lays = layers.Dropout(dropout_array[arc])(lays)\n",
    "    lays = layers.Dense(units=2, activation='softmax')(lays)\n",
    "    lays = layers.Dropout(dropout_array[arc])(lays)\n",
    "\n",
    "\n",
    "    model = models.Model(input_img, lays)\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    def get_lr_metric(optimizer):\n",
    "        def lr(y_true, y_pred):\n",
    "            return optimizer._decayed_lr(tf.float32) # I use ._decayed_lr method instead of .lr\n",
    "        return lr\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                        initial_learning_rate=learn_array[arc],\n",
    "                        decay_steps=10,\n",
    "                        decay_rate=decaylearn_array[arc])\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=lr_schedule,weight_decay=wdecay_array[arc])\n",
    "    lr_metric = get_lr_metric(opt)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        # Loss function to minimize\n",
    "        loss='binary_crossentropy',\n",
    "        # List of metrics to monitor\n",
    "        metrics=['mean_squared_error'],\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading training data and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.empty((0, div, div, chans))\n",
    "train_y = np.empty((0, div, div))\n",
    "for data_dir in map_list:\n",
    "    dir = train_data_dir+data_dir\n",
    "    assert os.path.exists(dir), f'data_dir {dir} does not exist'\n",
    "    train_x_aux = np.loadtxt(dir+'/train_X.csv')\n",
    "    sizes = train_x_aux[0:4].astype(int)\n",
    "    train_x_aux = np.delete(train_x_aux, [0,1,2,3])\n",
    "    train_x_aux = np.reshape(train_x_aux,sizes)\n",
    "\n",
    "    train_y_aux = np.loadtxt(dir+'/train_Y.csv')\n",
    "    sizes = train_y_aux[0:3].astype(int)\n",
    "    train_y_aux = np.delete(train_y_aux, [0,1,2])\n",
    "    train_y_aux = np.reshape(train_y_aux,sizes)\n",
    "\n",
    "    train_x = np.append(train_x, train_x_aux, axis=0)\n",
    "    train_y = np.append(train_y, train_y_aux, axis=0)\n",
    "\n",
    "val_x = np.empty((0, div, div, chans))\n",
    "val_y = np.empty((0, div, div))\n",
    "for data_dir in val_list:\n",
    "    dir = train_data_dir+data_dir\n",
    "    assert os.path.exists(dir), f'data_dir {dir} does not exist'\n",
    "    val_x_aux = np.loadtxt(dir+'/train_X.csv')\n",
    "    sizes = val_x_aux[0:4].astype(int)\n",
    "    val_x_aux = np.delete(val_x_aux, [0,1,2,3])\n",
    "    val_x_aux = np.reshape(val_x_aux,sizes)\n",
    "\n",
    "    val_y_aux = np.loadtxt(dir+'/train_Y.csv')\n",
    "    sizes = val_y_aux[0:3].astype(int)\n",
    "    val_y_aux = np.delete(val_y_aux, [0,1,2])\n",
    "    val_y_aux = np.reshape(val_y_aux,sizes)\n",
    "\n",
    "    val_x = np.append(val_x, val_x_aux, axis=0)\n",
    "    val_y = np.append(val_y, val_y_aux, axis=0)\n",
    "\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience=15),\n",
    "    # tf.keras.callbacks.ModelCheckpoint(filepath='IRI_models/model.{epoch:02d}-{val_loss:.2f}.h5'),\n",
    "    # tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    x=train_x,\n",
    "    y=np.stack((train_y,1-train_y),axis=3),\n",
    "    batch_size=600,\n",
    "    epochs=100, #100\n",
    "    validation_data=(val_x, np.stack((val_y,1-val_y),axis=3)),\n",
    "    callbacks=my_callbacks,\n",
    ")\n",
    "\n",
    "# if not os.path.exists(filename):\n",
    "model.save(filename+'_'+val_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux_x = validation_x\n",
    "# aux_y = validation_y\n",
    "\n",
    "# output = model.predict(aux_x,verbose=0)[:,:,:,0]\n",
    "\n",
    "# n = num_validation  # How many images we will display\n",
    "# plt.figure(figsize=(25, 5))\n",
    "# for i in range(n):\n",
    "#     # Display original\n",
    "#     ax = plt.subplot(2, n, i + 1)\n",
    "#     plt.imshow(np.multiply(np.stack((aux_x[i,:,:,0],aux_x[i,:,:,0],aux_x[i,:,:,0]),axis=2), np.stack((np.full(aux_y[i].shape,1),1-aux_y[i],1-aux_y[i]),axis=2)))\n",
    "#     ax.get_xaxis().set_visible(False)\n",
    "#     ax.get_yaxis().set_visible(False)\n",
    "\n",
    "#     # Display reconstruction\n",
    "#     ax = plt.subplot(2, n, i + 1 + n)\n",
    "#     plt.imshow(np.multiply(np.stack((aux_x[i,:,:,0],aux_x[i,:,:,0],aux_x[i,:,:,0]),axis=2), np.stack((np.full(output[i].shape,1),1-output[i],1-output[i]),axis=2)))\n",
    "#     ax.get_xaxis().set_visible(False)\n",
    "#     ax.get_yaxis().set_visible(False)\n",
    "# # Display original\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss, test_acc = model.evaluate(aux_x,  np.stack((val_y,1-val_y),axis=3), verbose=2)\n",
    "\n",
    "plt.plot(history.history['mean_squared_error'], label='mean_squared_error')\n",
    "plt.plot(history.history['val_mean_squared_error'], label = 'val_mean_squared_error')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label = 'loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTES"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x] Add benches, tables & chairs\n",
    "- [x] Add also velocity information (two different maps for velocities and regions of stop)\n",
    "- [x]    80% of time people pass stair and not sit\n",
    "- [x]    white noise velocity \n",
    "- [ ] Add heading of motion (directional velocity)\n",
    "- [ ] MAYBE Time of the day\n",
    "\n",
    "- [ ] mobility: main cues they are looking for (narrow places, which other criterias?) \"Criterium\" what do we need to look for\n",
    "(from computer vision)\n",
    "\n",
    "- [ ] collect data with following robot/static robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add the velocity and stop information is enough novelty?\n",
    "- Could the same network handle everything (all outputs: occupancy, velocity, stops)? First try seems not.. (only with occupancy and velocity.. but it could be my fault! binary crossentropy!!!)\n",
    "- Could same network handle both people, cars and bicycles?\n",
    "- Paper \"Learning Occupancy Priors of Human Motion From Semantic Maps of Urban Enviroments\" uses KL-divergence to compare to baselines.\n",
    "    They say that \"only a few methods explicitly highlight the performance in new environments outside the training scenario\"\n",
    "    Their future work section: \"Furthermore, we plan to validate semapp with on-the-fly semantics estimation and extend it to first-person view for application in automated driving to infer potential pedestriansâ€™ entrance points to the road.\"\n",
    "\n",
    "\n",
    "- [x] Add barriers: completely close passage or partially\n",
    "- [x] validate w/ simulation and exp (stanford dataset)\n",
    "- [ ] check reference of new paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mobility environment cues:\n",
    "- what is important in mobility: velocity, narrow spaces, individual or group\n",
    "- STOP is important"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ROS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
