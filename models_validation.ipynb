{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import skimage.measure\n",
    "\n",
    "import torch\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINED MODELS\n",
    "## Variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = 32\n",
    "step = int(32/4)\n",
    "red = 1\n",
    "\n",
    "map_list_t = ['master_big','master_big_closed','master_big_semiclosed']\n",
    "map_root_name = 'master_big'\n",
    "# map_list = ['map2','stanford_hyang10','master_big','willow','costacafe','map1','map3','stanford_coupa0', 'stanford_coupa3','stanford_hyang1','stanford_gates2']\n",
    "map_list = ['stanford_coupa0', 'stanford_coupa1', 'stanford_coupa2', 'stanford_coupa3', 'stanford_deathCircle0', 'stanford_gates2', 'stanford_hyang2', 'stanford_hyang3', 'stanford_hyang4', 'stanford_hyang10', 'stanford_little3', 'stanford_nexus0', 'stanford_nexus1']\n",
    "\n",
    "sem_dict = ['cash', 'entrance', 'light', 'sit', 'stairs', 'trash', 'tree','restricted','grass','intersection','shadow']\n",
    "chans = len(sem_dict)+1\n",
    "\n",
    "lut_in = [0, 20, 50, 100, 150, 255]\n",
    "lut_out = [0, 100, 180, 220, 240, 255]\n",
    "lut_8u = np.interp(np.arange(0, 256), lut_in, lut_out).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spec = 'stan'\n",
    "# date = '01feb'\n",
    "\n",
    "# model = tf.keras.models.load_model('IRI_models/'+date+'_'+str(step)+'px_steps_'+spec+'_paths')\n",
    "\n",
    "# model1 = tf.keras.models.load_model('IRI_models/'+date+'_8px_steps_'+spec+'_vels')\n",
    "\n",
    "# model2 = tf.keras.models.load_model('IRI_models/'+date+'_8px_steps_'+spec+'_stops')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing closed corridors (completely or partially closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for map_name in map_list_t:\n",
    "#   print(map_name)\n",
    "\n",
    "#   lines = 0\n",
    "#   with open('maps/semantics/'+map_root_name+'/'+map_name+'.csv') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "#   h = len(lines)\n",
    "#   w = len(lines[0].split(','))\n",
    "\n",
    "#   # Converts data to a list of integers\n",
    "#   map = []\n",
    "#   for line in lines:\n",
    "#     map.extend([int(c) for c in line.split(',')])\n",
    "\n",
    "#   for lab_class in sem_dict:\n",
    "#     lines = 0\n",
    "#     try:\n",
    "#       with open('maps/semantics/'+map_root_name+'/'+map_root_name+'_sem_'+lab_class+'.csv') as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "#       hh = len(lines)\n",
    "#       ww = len(lines[0].split(','))\n",
    "\n",
    "#       if hh != h or ww != w:\n",
    "#         print(f'h: {h}\\tw: {w}')\n",
    "#         print(f'h: {hh}\\tw: {ww}')\n",
    "#         raise SystemExit(\"ERROR: Different sizes!!\")\n",
    "\n",
    "#       # Converts data to a list of integers\n",
    "#       for line in lines:\n",
    "#         map.extend([int(c) for c in line.split(',')])\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#       for i in range(h):\n",
    "#         for j in range(w):\n",
    "#           map.extend([255])\n",
    "\n",
    "#   map = np.reshape(map,[chans,h,w])\n",
    "#   map = np.moveaxis(map, 0, -1)\n",
    "#   map = map/255\n",
    "\n",
    "#   map_aux = map\n",
    "#   map = np.zeros((int(math.ceil(h/2)),int(math.ceil(w/2)),chans))\n",
    "\n",
    "#   for idx in range(chans):\n",
    "#     map[:,:,idx] = skimage.measure.block_reduce(map_aux[:,:,idx], (2,2), np.max)\n",
    "#   h, w, _ = map.shape\n",
    "\n",
    "#   diff_h = int((h-div*int(h/div))/2)\n",
    "#   r_h = int((h-div*int(h/div))%2) + diff_h\n",
    "#   diff_w = int((w-div*int(w/div))/2)\n",
    "#   r_w = int((w-div*int(w/div))%2) + diff_w\n",
    "#   map = map[r_h:-diff_h:,r_w+diff_w:,:]\n",
    "\n",
    "#   # print(map.shape)\n",
    "#   h, w, _ = map.shape\n",
    "\n",
    "#   # -----------------------------------------------------------------------------------\n",
    "#   # creating subplot and figure\n",
    "#   fig = plt.figure(figsize=(w/70,h/70))\n",
    "#   data_pred = np.zeros((int(math.ceil(h)),int(math.ceil(w))))\n",
    "\n",
    "#   step = int(32/4)\n",
    "#   for i in np.arange((w/step+int(div/step-1))*(h/step+int(div/step-1))):\n",
    "#     c = int(i%(w/step+int(div/step-1))) - int(div/step-1)*0\n",
    "#     r = int(i/(w/step+int(div/step-1))) - int(div/step-1)*0\n",
    "#     submap = map[max(step*r,0):step*r+div, max(step*c,0):step*c+div,:]\n",
    "#     subdata = model.predict(np.expand_dims(submap,axis=0),verbose=0)[:,:,:,0]\n",
    "#     subdata = np.squeeze(subdata,axis=0)\n",
    "#     data_pred[max(step*r,0):step*r+div, max(step*c,0):step*c+div] += subdata*1/(int(div/step)*int(div/step))\n",
    "#   contrasted_data = cv2.LUT((data_pred/np.max(data_pred)*255).astype(np.uint8), lut_8u).astype(float)/255\n",
    "#   plt.imshow(np.multiply(np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2), np.stack((np.full(contrasted_data.shape,1),1-contrasted_data,1-contrasted_data),axis=2)), vmin=0, vmax=1)\n",
    "#   # plt.imshow(np.multiply(np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2), np.stack((np.full(data_pred.shape,1),1-data_pred,1-data_pred),axis=2)), vmin=0, vmax=1)\n",
    "#   plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl1 = []\n",
    "\n",
    "for map_count, map_name in enumerate(map_list):\n",
    "  print(map_name)\n",
    "\n",
    "  model = tf.keras.models.load_model('IRI_models/final_'+map_name)\n",
    "\n",
    "  lines = 0\n",
    "  try:\n",
    "    with open('maps/semantics/'+map_name+'/'+map_name+'-reduced.csv') as f:\n",
    "      print('Reduced map found')\n",
    "      lines = f.readlines()\n",
    "      \n",
    "  except FileNotFoundError:\n",
    "    with open('maps/semantics/'+map_name+'/'+map_name+'.csv') as f:\n",
    "      print('Using original map')\n",
    "      lines = f.readlines()\n",
    "\n",
    "  h = len(lines)\n",
    "  w = len(lines[0].split(','))\n",
    "\n",
    "  # Converts data to a list of integers\n",
    "  map = []\n",
    "  for line in lines:\n",
    "    map.extend([int(c) for c in line.split(',')])\n",
    "\n",
    "  for lab_class in sem_dict:\n",
    "    lines = 0\n",
    "    try:\n",
    "      with open('maps/semantics/'+map_name+'/'+map_name+'_sem_'+lab_class+'-reduced.csv') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "      hh = len(lines)\n",
    "      ww = len(lines[0].split(','))\n",
    "\n",
    "      if hh != h or ww != w:\n",
    "        print(f'h: {h}\\tw: {w}')\n",
    "        print(f'h: {hh}\\tw: {ww}')\n",
    "        raise SystemExit(\"ERROR: Different sizes!!\")\n",
    "\n",
    "      # Converts data to a list of integers\n",
    "      for line in lines:\n",
    "        map.extend([int(c) for c in line.split(',')])\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "      try:\n",
    "        with open('maps/semantics/'+map_name+'/'+map_name+'_sem_'+lab_class+'.csv') as f:\n",
    "          lines = f.readlines()\n",
    "\n",
    "        hh = len(lines)\n",
    "        ww = len(lines[0].split(','))\n",
    "\n",
    "        if hh != h or ww != w:\n",
    "          print(f'h: {h}\\tw: {w}')\n",
    "          print(f'h: {hh}\\tw: {ww}')\n",
    "          raise SystemExit(\"ERROR: Different sizes!!\")\n",
    "\n",
    "        # Converts data to a list of integers\n",
    "        for line in lines:\n",
    "          map.extend([int(c) for c in line.split(',')])\n",
    "\n",
    "      except FileNotFoundError:\n",
    "        for i in range(h):\n",
    "          for j in range(w):\n",
    "            map.extend([255])\n",
    "\n",
    "  map = np.reshape(map,[chans,h,w])\n",
    "  map = np.moveaxis(map, 0, -1)\n",
    "  map = map/255\n",
    "\n",
    "  map_aux = map\n",
    "  map = np.zeros((int(math.ceil(h/red)),int(math.ceil(w/red)),chans))\n",
    "\n",
    "  for idx in range(chans):\n",
    "    map[:,:,idx] = skimage.measure.block_reduce(map_aux[:,:,idx], (red,red), np.max)\n",
    "\n",
    "  # print(map.shape)\n",
    "  h, w, _ = map.shape\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "  lines = 0\n",
    "  with open('maps/semantics/'+map_name+'/humandensity-'+map_name+'-new.csv') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "  hd = len(lines)\n",
    "  wd = len(lines[0].split(','))\n",
    "\n",
    "  # Converts data to a list of integers\n",
    "  data = []\n",
    "  for line in lines:\n",
    "    data.extend([int(c) for c in line.split(',')])\n",
    "\n",
    "  data = np.reshape(data,[hd,wd])\n",
    "  sigma = 10.0\n",
    "  # data = skimage.filters.gaussian(data, sigma=(sigma, sigma), channel_axis=-1)\n",
    "  data = skimage.measure.block_reduce(data, (red,red), np.max)\n",
    "  # data = np.subtract(data, np.full((h, w), np.min(data)))/(np.max(data)-np.min(data))\n",
    "  data = data*(map[:,:,0]>0)\n",
    "\n",
    "  sigma = 0.7\n",
    "  data_show = data\n",
    "  # data_show = cv2.LUT((data_show*255).astype(np.uint8), lut_8u).astype(np.float32)/255\n",
    "  data_show = skimage.filters.gaussian(data_show, sigma=(sigma, sigma), channel_axis=-1)\n",
    "  data_show = skimage.filters.gaussian(data_show, sigma=(sigma, sigma), channel_axis=-1)\n",
    "  data = data_show/np.sum(data_show)\n",
    "  data_show = data_show/np.max(data_show)\n",
    "\n",
    "  # print(data.shape)\n",
    "  hd, wd = data.shape\n",
    "\n",
    "  print(sem_dict)\n",
    "  plt.figure(figsize=(20,5))\n",
    "  for i in range(len(sem_dict)):\n",
    "    ax = plt.subplot(1, len(sem_dict), i+1)\n",
    "    alp = 0.5\n",
    "    ax.imshow(np.multiply(np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2),np.multiply(np.stack((map[:,:,i+1],map[:,:,i+1],map[:,:,i+1]),axis=2)*alp+(1-alp), np.stack((np.full(data.shape,1),1-data,1-data),axis=2))), vmin=0, vmax=1)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "  plt.show()\n",
    "\n",
    "  ax = plt.subplot(111)\n",
    "  ax.imshow(np.multiply(np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2), np.stack((np.full(data_show.shape,1),1-data_show,1-data_show),axis=2)), vmin=0, vmax=1)\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "  plt.show()\n",
    "\n",
    "  # -----------------------------------------------------------------------------------\n",
    "  # Random sample 32x32 windows in map\n",
    "  n_crops = 500\n",
    "  inserted = 0\n",
    "  crops = np.zeros((n_crops,2),dtype=int)\n",
    "  selections = np.zeros((h,w))\n",
    "  for i in range(n_crops):\n",
    "    background = np.zeros((h,w))\n",
    "    flag = True\n",
    "    while flag:\n",
    "      aux_x = int(random.random()*(w-div+1))\n",
    "      aux_y = int(random.random()*(h-div+1))\n",
    "      flag = False\n",
    "      # print(f'INSERTED: {inserted}  ->  x: {aux_x} and y: {aux_y}')\n",
    "      # for z in range(inserted):\n",
    "      #   if abs(aux_x-crops[z,0])<int(div/10) and abs(aux_y-crops[z,1])<int(div/10):\n",
    "      #     # print('too close')\n",
    "      #     flag = True\n",
    "    crops[i,0] = aux_x\n",
    "    crops[i,1] = aux_y\n",
    "    selections = selections+cv2.rectangle(background,(crops[i,0],crops[i,1]),(crops[i,0]+div-1,crops[i,1]+div-1),(1,0,0),-1)\n",
    "    inserted = inserted + 1\n",
    "  selections = 1/selections\n",
    "\n",
    "  # -----------------------------------------------------------------------------------\n",
    "  # creating subplot and figure\n",
    "  # ax = plt.subplot(111)\n",
    "  data_pred = np.zeros((int(math.ceil(h)),int(math.ceil(w))))\n",
    "  kls = []\n",
    "  for i in range(n_crops):\n",
    "    submap = map[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div,:]\n",
    "    data_aux = data[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "    subdata = model.predict(np.expand_dims(submap,axis=0),verbose=0)[:,:,:,0]\n",
    "    subdata = np.squeeze(subdata,axis=0)\n",
    "    data_pred[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div] += subdata*selections[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "    # Calculate the KL-divergence\n",
    "    \n",
    "  data = torch.Tensor(data)\n",
    "  data_pred = torch.Tensor(data_pred)\n",
    "  data = data/data.sum()\n",
    "  data_pred = data_pred/data_pred.sum()\n",
    "  data_pred[data_pred == 0] = 1e-12\n",
    "  data_pred[data_pred == 1] = 1-1e-12\n",
    "  data[data == 0] = 1e-12\n",
    "  data[data == 1] = 1-1e-12\n",
    "\n",
    "  kl = torch.nn.functional.kl_div(data_pred.unsqueeze(0).log(), data.unsqueeze(0), reduction='batchmean').item()\n",
    "\n",
    "  data = data/data.max()\n",
    "  data_pred = data_pred/data_pred.max()\n",
    "  plt.figure(figsize=(4,4))\n",
    "  plt.imshow(np.multiply(np.stack((1-data,1-data,np.full(data.shape,1)),axis=2), np.stack((np.full(data_pred.shape,1),1-data_pred,1-data_pred),axis=2)))\n",
    "  plt.show()\n",
    "  \n",
    "  data_pred = data_pred/np.max(data_pred.detach().cpu().numpy())\n",
    "  plt.imshow(np.multiply(np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2), np.stack((np.full(data_pred.shape,1),1-data_pred,1-data_pred),axis=2)), vmin=0, vmax=1)\n",
    "  # plt.imshow(np.multiply(np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2), np.stack((np.full(data_pred.shape,1),1-data_pred,1-data_pred),axis=2)), vmin=0, vmax=1)\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "  plt.show()\n",
    "\n",
    "  print(f'KL-divergence: {kl}')\n",
    "  kl1 = np.append(kl1,kl)\n",
    "  import sys\n",
    "  sys.exit(0)\n",
    "\n",
    "print('-----------------------------')\n",
    "print(f'Mean KL-divergence: {np.mean(kl1)}')\n",
    "print(f'Std KL-divergence: {np.std(kl1)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
