{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import skimage.measure\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = 64\n",
    "step = int(32/4)\n",
    "red = 1\n",
    "\n",
    "# Random sample 32x32 windows in map\n",
    "n_crops = 500  #500\n",
    "\n",
    "map_list = ['stanford_coupa0', 'stanford_coupa1', 'stanford_coupa2', 'stanford_coupa3', 'stanford_deathCircle0', 'stanford_gates2', 'stanford_hyang2', 'stanford_hyang3', 'stanford_hyang4', 'stanford_hyang10', 'stanford_little3', 'stanford_nexus0', 'stanford_nexus1']\n",
    "# semantic class cash is used also for bike parking spots\n",
    "# sem_dict = ['cash', 'entrance', 'light', 'sit', 'stairs', 'trash', 'tree','restricted','grass','intersection','shadow']\n",
    "sem_dict = ['bicycle_road', 'building', 'entrance', 'grass', 'obstacle', 'parking', 'pedestrian_road', 'tree', 'vehicle_road']\n",
    "chans = len(sem_dict)\n",
    "\n",
    "lut_in = [0, 10, 30, 50, 70, 110, 150, 255]\n",
    "lut_out = [0, 120, 200, 225, 240, 248, 250, 255]\n",
    "lut_in = [0, 10, 70, 150, 255]\n",
    "lut_out = [0, 130, 210, 245, 255]\n",
    "# plt.plot(lut_in, lut_out)\n",
    "lut_8u = np.interp(np.arange(0, 256), lut_in, lut_out).astype(np.uint8)\n",
    "\n",
    "train_x = np.zeros((1, div, div, chans))\n",
    "train_y = np.zeros((1, div, div))\n",
    "train_y1 = np.zeros((1, div, div))\n",
    "train_y2 = np.zeros((1, div, div))\n",
    "\n",
    "spec = 'stan'\n",
    "train_data_dir = 'training_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "for map_count, map_name in enumerate(map_list):\n",
    "  print(map_name)\n",
    "\n",
    "  map = []\n",
    "  for lab_class in sem_dict:\n",
    "    lines = 0\n",
    "    try:\n",
    "      with open('maps/semantics/'+map_name+'/'+map_name+'_sem_'+lab_class+'-reduced.csv') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "      h = len(lines)\n",
    "      w = len(lines[0].split(','))\n",
    "\n",
    "      # Converts data to a list of integers\n",
    "      for line in lines:\n",
    "        map.extend([int(c) for c in line.split(',')])\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "      try:\n",
    "        with open('maps/semantics/'+map_name+'/'+map_name+'_sem_'+lab_class+'.csv') as f:\n",
    "          lines = f.readlines()\n",
    "\n",
    "        h = len(lines)\n",
    "        w = len(lines[0].split(','))\n",
    "\n",
    "        # Converts data to a list of integers\n",
    "        for line in lines:\n",
    "          map.extend([int(c) for c in line.split(',')])\n",
    "\n",
    "      except FileNotFoundError:\n",
    "        for i in range(h):\n",
    "          for j in range(w):\n",
    "            map.extend([255])\n",
    "\n",
    "\n",
    "  map = np.reshape(map,[chans,h,w])\n",
    "  map = np.moveaxis(map, 0, -1)\n",
    "  map = map/255\n",
    "\n",
    "  map_aux = map\n",
    "  map = np.zeros((int(math.ceil(h/red)),int(math.ceil(w/red)),chans))\n",
    "\n",
    "  for idx in range(chans):\n",
    "    map[:,:,idx] = skimage.measure.block_reduce(map_aux[:,:,idx], (red,red), np.max)\n",
    "\n",
    "  # print(map.shape)\n",
    "  h, w, _ = map.shape\n",
    "\n",
    "  # -----------------------------------------------------------------------------------\n",
    "  lines = 0\n",
    "  with open('maps/semantics/'+map_name+'/humandensity-'+map_name+'-new.csv') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "  hd = len(lines)\n",
    "  wd = len(lines[0].split(','))\n",
    "\n",
    "  # Converts data to a list of integers\n",
    "  data = []\n",
    "  for line in lines:\n",
    "    data.extend([int(c) for c in line.split(',')])\n",
    "\n",
    "  data = np.reshape(data,[hd,wd])\n",
    "  sigma = 10.0\n",
    "  # old_max = np.max(data)\n",
    "  # data = skimage.filters.gaussian(data, sigma=(sigma, sigma), channel_axis=-1)\n",
    "  # data = data/np.max(data)*old_max\n",
    "  data = skimage.measure.block_reduce(data, (red,red), np.max)\n",
    "  # data = np.subtract(data, np.full((h, w), np.min(data)))/(np.max(data)-np.min(data))\n",
    "  data = data*(map[:,:,0]>0)\n",
    "\n",
    "  # print(data.shape)\n",
    "  hd, wd = data.shape\n",
    "  \n",
    "  sigma = 1.5\n",
    "  data_show = data\n",
    "  # data_show = cv2.LUT((data_show*255).astype(np.uint8), lut_8u).astype(np.float32)/255\n",
    "  data_show = skimage.filters.gaussian(data_show, sigma=(sigma, sigma), channel_axis=-1)\n",
    "  data_show = skimage.filters.gaussian(data_show, sigma=(sigma, sigma), channel_axis=-1)\n",
    "  data = data_show/np.sum(data_show)\n",
    "  data_show = data_show/np.max(data_show)\n",
    "  fig = plt.figure(figsize=(5,5))\n",
    "  plt.imshow(data_show)\n",
    "  plt.show()\n",
    "\n",
    "  sem_map = np.zeros((h,w,3))\n",
    "  colors = [[255,204,51],[189,189,189],[255,255,255],[61,245,61],[255,0,0],[255,0,204],[42,125,209],[9,117,45],[250,50,83]]\n",
    "  for i in range(len(sem_dict)):\n",
    "    sem = np.full((h,w,3),colors[i])\n",
    "    sem_map = np.stack((1-map[:,:,i],1-map[:,:,i],1-map[:,:,i]), axis=2)*sem+sem_map\n",
    "  plt.imshow(sem_map/255)\n",
    "  plt.show()\n",
    "\n",
    "  # # -----------------------------------------------------------------------------------\n",
    "  # lines = 0\n",
    "  # with open('maps/semantics/'+map_name+'/humandensity-'+map_name+'-vel.csv') as f:\n",
    "  #   lines = f.readlines()\n",
    "\n",
    "  # hd = len(lines)\n",
    "  # wd = len(lines[0].split(','))\n",
    "\n",
    "  # # Converts data to a list of integers\n",
    "  # data1 = []\n",
    "  # for line in lines:\n",
    "  #   data1.extend([float(c) for c in line.split(',')])\n",
    "\n",
    "  # data1 = np.reshape(data1,[hd,wd])\n",
    "  # sigma = 10.0\n",
    "  # data1 = skimage.filters.gaussian(data1, sigma=(sigma, sigma), channel_axis=-1)\n",
    "  # data1 = skimage.measure.block_reduce(data1, (red,red), np.max)\n",
    "  # data1 = np.subtract(data1, np.full((h, w), np.min(data1)))/(np.max(data1)-np.min(data1))\n",
    "\n",
    "  # # print(data1.shape)\n",
    "  # hd, wd = data1.shape\n",
    "\n",
    "  # # fig = plt.figure(figsize=(5,5))\n",
    "  # # plt.imshow(np.multiply(np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2), np.stack((np.full(data1.shape,1),1-data1,1-data1),axis=2)), vmin=0, vmax=1)\n",
    "  # # plt.show()\n",
    "\n",
    "  # # -----------------------------------------------------------------------------------\n",
    "  # lines = 0\n",
    "  # with open('maps/semantics/'+map_name+'/humandensity-'+map_name+'-stop.csv') as f:\n",
    "  #   lines = f.readlines()\n",
    "\n",
    "  # hd = len(lines)\n",
    "  # wd = len(lines[0].split(','))\n",
    "\n",
    "  # # Converts data to a list of integers\n",
    "  # data2 = []\n",
    "  # for line in lines:\n",
    "  #   data2.extend([float(c) for c in line.split(',')])\n",
    "\n",
    "  # data2 = np.reshape(data2,[hd,wd])\n",
    "  # sigma = 10.0\n",
    "  # data2 = skimage.filters.gaussian(data2, sigma=(sigma, sigma), channel_axis=-1)\n",
    "  # data2 = skimage.measure.block_reduce(data2, (red,red), np.max)\n",
    "  # data2 = np.subtract(data2, np.full((h, w), np.min(data2)))/(np.max(data2)-np.min(data2))\n",
    "\n",
    "  # # print(data2.shape)\n",
    "  # hd, wd = data2.shape\n",
    "\n",
    "  # # fig = plt.figure(figsize=(5,5))\n",
    "  # # plt.imshow(np.multiply(np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2), np.stack((np.full(data2.shape,1),1-data2,1-data2),axis=2)), vmin=0, vmax=1)\n",
    "  # # plt.show()\n",
    "\n",
    "  # -----------------------------------------------------------------------------------\n",
    "  # fig = plt.figure(figsize=(5,5))\n",
    "  data_pred = np.zeros((int(math.ceil(h)),int(math.ceil(w))))\n",
    "  # data_pred1 = np.zeros((int(math.ceil(h)),int(math.ceil(w))))\n",
    "  # data_pred2 = np.zeros((int(math.ceil(h)),int(math.ceil(w))))\n",
    "\n",
    "  num = 0\n",
    "  numv = 0\n",
    "  start = train_x.shape[0] - 1\n",
    "\n",
    "  inserted = 0\n",
    "  crops = np.zeros((n_crops,2),dtype=int)\n",
    "  selections = np.zeros((h,w))\n",
    "  for i in range(n_crops):\n",
    "    background = np.zeros((h,w))\n",
    "    flag = True\n",
    "    while flag:\n",
    "      aux_x = int(random.random()*(w-div+1))\n",
    "      aux_y = int(random.random()*(h-div+1))\n",
    "      flag = False\n",
    "\n",
    "      submap = map[aux_y:aux_y+div, aux_x:aux_x+div,:]\n",
    "      subdata = data[aux_y:aux_y+div, aux_x:aux_x+div]\n",
    "      # subdata1 = data1[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "      # subdata2 = data2[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "      # subdata = np.divide(subdata,np.sum(subdata))\n",
    "\n",
    "      if not (np.mean(submap) > 0.2 and np.mean(submap) < 1 and np.mean(subdata) > 0 and submap.shape == (div, div, chans)):\n",
    "        print('DISCARDED: not enough information')\n",
    "        flag = True\n",
    "        # test11 = 1-cv2.rectangle(background,(aux_x,aux_y),(aux_x+div-1,aux_y+div-1),(1,0,0),-1)/1.5\n",
    "        # test11 = np.multiply(np.stack((test11,test11,test11),axis=2),np.stack((np.full(data.shape,1),1-data,1-data),axis=2))\n",
    "        # test11 = np.multiply(test11,np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2))\n",
    "        \n",
    "        # for i in range(len(sem_dict)):\n",
    "        #     alp = 0.5\n",
    "        #     test11 = np.multiply(test11,np.stack((map[:,:,i+1],map[:,:,i+1],map[:,:,i+1]),axis=2)*alp+(1-alp))\n",
    "        # plt.figure(figsize=(5,5))\n",
    "        # plt.imshow(test11)\n",
    "\n",
    "        # print(f'\\nsubmap mean: {np.mean(submap)}')\n",
    "        # print(f'subdata mean: {np.mean(subdata)}')\n",
    "        # print(f'submap shape: {submap.shape}')\n",
    "        # import sys\n",
    "        # sys.exit(0)\n",
    "      # print(f'INSERTED: {inserted}  ->  x: {aux_x} and y: {aux_y}')\n",
    "      # for z in range(inserted):\n",
    "      #   if abs(aux_x-crops[z,0])<int(div/10) and abs(aux_y-crops[z,1])<int(div/10):\n",
    "      #     # print('too close')\n",
    "      #     flag = True\n",
    "        \n",
    "    crops[i,0] = aux_x\n",
    "    crops[i,1] = aux_y\n",
    "    selections = selections+cv2.rectangle(background,(crops[i,0],crops[i,1]),(crops[i,0]+div-1,crops[i,1]+div-1),(1,0,0),-1)\n",
    "    inserted = inserted + 1\n",
    "\n",
    "    num = num + 1\n",
    "    train_x = np.append(train_x, np.expand_dims(submap, axis=0), axis=0)\n",
    "    train_y = np.append(train_y, np.expand_dims(subdata, axis=0), axis=0)\n",
    "    # train_y1 = np.append(train_y1, np.expand_dims(subdata1, axis=0), axis=0)\n",
    "    # train_y2 = np.append(train_y2, np.expand_dims(subdata2, axis=0), axis=0)\n",
    "\n",
    "  selections = 1/selections\n",
    "\n",
    "  for i in range(n_crops):\n",
    "    submap = map[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div, :]\n",
    "    subdata = data[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "    if np.max(subdata) > 1 or np.min(subdata) < 0:\n",
    "      print(f'subdata max: {np.max(subdata)}')\n",
    "      print(f'subdata min: {np.min(subdata)}')\n",
    "    data_pred[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div] += subdata*selections[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "    # data_pred1[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div] += subdata1*selections[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "    # data_pred2[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div] += subdata2*selections[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "  \n",
    "  data_show = data_pred/np.max(data_pred)\n",
    "\n",
    "  # lim_val = 0.2\n",
    "  # data_show = np.clip(data,0,np.max(data)*lim_val)/(np.max(data)*lim_val)\n",
    "  # data_show = cv2.LUT((data_show*255).astype(np.uint8), lut_8u).astype(np.float32)/255\n",
    "  fig = plt.figure(figsize=(5,5))\n",
    "  plt.imshow(data_show)\n",
    "  plt.show()\n",
    "  \n",
    "  # plt.imshow(np.multiply(np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2), np.stack((np.full(data_pred1.shape,1),1-data_pred1,1-data_pred1),axis=2)))\n",
    "  # plt.show()\n",
    "  # plt.imshow(np.multiply(np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2), np.stack((np.full(data_pred2.shape,1),1-data_pred2,1-data_pred2),axis=2)))\n",
    "  # plt.show()\n",
    "\n",
    "  # -----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "  for i in np.arange(num)+start:\n",
    "    if i % 100:\n",
    "      print(i)\n",
    "    for j in np.arange(3):\n",
    "      train_x = np.append(train_x, np.expand_dims(np.rot90(train_x[i,:,:], k=j+1, axes=(0, 1)),axis=0), axis=0)\n",
    "      train_y = np.append(train_y, np.expand_dims(np.rot90(train_y[i,:,:], k=j+1, axes=(0, 1)),axis=0), axis=0)\n",
    "      # train_y1 = np.append(train_y1, np.expand_dims(np.rot90(train_y1[i,:,:], k=j+1, axes=(0, 1)),axis=0), axis=0)\n",
    "      # train_y2 = np.append(train_y2, np.expand_dims(np.rot90(train_y2[i,:,:], k=j+1, axes=(0, 1)),axis=0), axis=0)\n",
    "    \n",
    "    train_x = np.append(train_x, np.expand_dims(np.flip(train_x[i,:,:], axis=0), axis=0),axis=0)\n",
    "    train_x = np.append(train_x, np.expand_dims(np.flip(train_x[i,:,:], axis=1), axis=0),axis=0)\n",
    "    train_y = np.append(train_y, np.expand_dims(np.flip(train_y[i,:,:], axis=0), axis=0),axis=0)\n",
    "    train_y = np.append(train_y, np.expand_dims(np.flip(train_y[i,:,:], axis=1), axis=0),axis=0)\n",
    "    # train_y1 = np.append(train_y1, np.expand_dims(np.flip(train_y1[i,:,:], axis=0), axis=0),axis=0)\n",
    "    # train_y1 = np.append(train_y1, np.expand_dims(np.flip(train_y1[i,:,:], axis=1), axis=0),axis=0)\n",
    "    # train_y2 = np.append(train_y2, np.expand_dims(np.flip(train_y2[i,:,:], axis=0), axis=0),axis=0)\n",
    "    # train_y2 = np.append(train_y2, np.expand_dims(np.flip(train_y2[i,:,:], axis=1), axis=0),axis=0)\n",
    "\n",
    "  ############################################################################################################\n",
    "  ############################################################################################################\n",
    "  ############################################################################################################\n",
    "\n",
    "  train_x = np.delete(train_x, 0, 0)\n",
    "  train_y = np.delete(train_y, 0, 0)\n",
    "  train_y1 = np.delete(train_y1, 0, 0)\n",
    "  train_y2 = np.delete(train_y2, 0, 0)\n",
    "\n",
    "  print(train_x.shape)\n",
    "  \n",
    "  np.savetxt(train_data_dir+'/'+str(div)+'crop_size/'+str(len(sem_dict))+'labels/'+str(red)+'red/'+map_name+'/train_X.csv', np.insert(train_x.flatten(),0,train_x.shape), delimiter=',', fmt='%f')\n",
    "  np.savetxt(train_data_dir+'/'+str(div)+'crop_size/'+str(len(sem_dict))+'labels/'+str(red)+'red/'+map_name+'/train_Y.csv', np.insert(train_y.flatten(),0,train_y.shape), delimiter=',', fmt='%f')\n",
    "\n",
    "  train_x = np.zeros((1, div, div, chans))\n",
    "  train_y = np.zeros((1, div, div))\n",
    "  train_y1 = np.zeros((1, div, div))\n",
    "  train_y2 = np.zeros((1, div, div))\n",
    "\n",
    "  ############################################################################################################\n",
    "  ############################################################################################################\n",
    "  ############################################################################################################\n",
    "\n",
    "# # Show the random crop selection of the map\n",
    "# fig = plt.figure(figsize=(w/12,h/12))\n",
    "# plt.imshow(selections, vmin=0, vmax=1, cmap='magma')\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "\n",
    "train_x = np.delete(train_x, 0, 0)\n",
    "train_y = np.delete(train_y, 0, 0)\n",
    "train_y1 = np.delete(train_y1, 0, 0)\n",
    "train_y2 = np.delete(train_y2, 0, 0)\n",
    "\n",
    "print(train_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_validation = 0.5\n",
    " \n",
    "# original_data_x = train_x\n",
    "# original_data_y = train_y\n",
    "# original_data_y1 = train_y1\n",
    "# original_data_y2 = train_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = original_data_x\n",
    "# train_y = original_data_y\n",
    "# # train_y1 = original_data_y1\n",
    "# # train_y2 = original_data_y2\n",
    "\n",
    "# p = np.random.permutation(len(train_x))\n",
    "# train_x = train_x[p]\n",
    "# train_y = train_y[p]\n",
    "# # train_y1 = train_y1[p]\n",
    "# # train_y2 = train_y2[p]\n",
    "\n",
    "# num = train_x.shape[0]\n",
    "# num_test = int(num*num_validation)\n",
    "\n",
    "# test_x = train_x[:num_test,:,:]\n",
    "# test_y = train_y[:num_test,:,:]\n",
    "# # test_y1 = train_y1[:num_test,:,:]\n",
    "# # test_y2 = train_y2[:num_test,:,:]\n",
    "\n",
    "# train_x = train_x[num_test:,:,:]\n",
    "# train_y = train_y[num_test:,:,:]\n",
    "# # train_y1 = train_y1[num_test:,:,:]\n",
    "# # train_y2 = train_y2[num_test:,:,:]\n",
    "\n",
    "# print(train_x.shape)\n",
    "# print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(train_data_dir+'/train_X.csv', np.insert(train_x.flatten(),0,train_x.shape), delimiter=',', fmt='%f')\n",
    "# np.savetxt(train_data_dir+'/train_Y.csv', np.insert(train_y.flatten(),0,train_y.shape), delimiter=',', fmt='%f')\n",
    "# # np.savetxt(train_data_dir+'/train_Y1.csv', np.insert(train_y1.flatten(),0,train_y1.shape), delimiter=',', fmt='%f')\n",
    "# # np.savetxt(train_data_dir+'/train_Y2.csv', np.insert(train_y2.flatten(),0,train_y2.shape), delimiter=',', fmt='%f')\n",
    "\n",
    "# np.savetxt(train_data_dir+'/test_X.csv', np.insert(test_x.flatten(),0,test_x.shape), delimiter=',', fmt='%f')\n",
    "# np.savetxt(train_data_dir+'/test_Y.csv', np.insert(test_y.flatten(),0,test_y.shape), delimiter=',', fmt='%f')\n",
    "# # np.savetxt(train_data_dir+'/test_Y1.csv', np.insert(test_y1.flatten(),0,test_y1.shape), delimiter=',', fmt='%f')\n",
    "# # np.savetxt(train_data_dir+'/test_Y2.csv', np.insert(test_y2.flatten(),0,test_y2.shape), delimiter=',', fmt='%f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = int(random.random()*test_x.shape[0])\n",
    "# # i = 6359\n",
    "# print(i)\n",
    "# submap = test_x[i,:,:]\n",
    "# subdata = test_y[i,:,:]\n",
    "\n",
    "# print(sem_dict)\n",
    "# plt.figure(figsize=(10,5))\n",
    "# for i in range(len(sem_dict)):\n",
    "#     ax = plt.subplot(1, len(sem_dict), i+1)\n",
    "#     alp = 0.5\n",
    "#     ax.imshow(np.multiply(np.stack((submap[:,:,i+1],submap[:,:,i+1],submap[:,:,i+1]),axis=2)*alp+(1-alp), np.stack((np.full(subdata.shape,1),1-subdata,1-subdata),axis=2)))\n",
    "#     ax.get_xaxis().set_visible(False)\n",
    "#     ax.get_yaxis().set_visible(False)\n",
    "# plt.show()\n",
    "# fig = plt.figure(figsize=(2,2))\n",
    "# contrasted_subdata = cv2.LUT((subdata/np.max(subdata)*255).astype(np.uint8), lut_8u).astype(float)/255\n",
    "# plt.imshow(np.multiply(np.stack((submap[:,:,0],submap[:,:,0],submap[:,:,0]),axis=2), np.stack((np.full(subdata.shape,1),1-subdata,1-subdata),axis=2)))\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
