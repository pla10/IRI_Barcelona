{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import skimage.measure\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# ------------------------\n",
    "# Configuration Parameters\n",
    "# ------------------------\n",
    "\n",
    "div = 64  # Size of the square crop windows\n",
    "red = 1   # Reduction factor\n",
    "resize_ratio = 1  # Ratio to resize maps\n",
    "n_crops = 300  # Number of random crops per map\n",
    "chans = 13    # Number of semantic classes\n",
    "\n",
    "# Box filter kernels\n",
    "box = 4\n",
    "kernel = np.ones((box,box),np.float32)/(box**2)\n",
    "boxp = 8\n",
    "kernelp = np.ones((boxp,boxp),np.float32)/(boxp**2)\n",
    "\n",
    "# ------------------------\n",
    "# Main Processing Function\n",
    "# ------------------------\n",
    "\n",
    "def process_map(map_name):\n",
    "    \"\"\"\n",
    "    Extracts training samples from a single map.\n",
    "\n",
    "    Args:\n",
    "        map_name (str): Name of the map (e.g., 'stanford_bookstore0')\n",
    "    \"\"\"\n",
    "\n",
    "    print(map_name)\n",
    "\n",
    "    # Training data initialization\n",
    "    train_x = np.zeros((1, div, div, chans))\n",
    "    train_y = np.zeros((1, div, div))\n",
    "    train_y1 = np.zeros((1, div, div))\n",
    "    train_y2 = np.zeros((1, div, div))\n",
    "\n",
    "    train_data_dir = '/data/placido/training_data'\n",
    "\n",
    "    # ------------------------\n",
    "    # Load Maps\n",
    "    # ------------------------\n",
    "\n",
    "    hd_path = f'/home/placido.falqueto/IRI_Barcelona/maps/13semantics/{map_name}/humandensity-{map_name}-new.csv'\n",
    "    vel_path = f'/home/placido.falqueto/IRI_Barcelona/maps/13semantics/{map_name}/humandensity-{map_name}-vel.csv'\n",
    "    stop_path = f'/home/placido.falqueto/IRI_Barcelona/maps/13semantics/{map_name}/humandensity-{map_name}-stop.csv'\n",
    "    png_path = f'/home/placido.falqueto/IRI_Barcelona/maps/13semantics/{map_name}/{map_name}_colors.png'\n",
    "    labelmap_path = '/home/placido.falqueto/IRI_Barcelona/maps/13semantics/labelmap.txt'\n",
    "\n",
    "    # Load human density, velocity, and stop maps\n",
    "    data = np.genfromtxt(hd_path, delimiter=',')\n",
    "    data1 = np.genfromtxt(vel_path, delimiter=',')\n",
    "    data2 = np.genfromtxt(stop_path, delimiter=',')\n",
    "\n",
    "    # Downsample maps\n",
    "    data = skimage.measure.block_reduce(data, (red, red), np.max)\n",
    "    data1 = skimage.measure.block_reduce(data1, (red, red), np.max)\n",
    "    data2 = skimage.measure.block_reduce(data2, (red, red), np.max)\n",
    "\n",
    "    # Resize maps if needed\n",
    "    data = cv2.resize(data, None, fx=resize_ratio, fy=resize_ratio, interpolation=cv2.INTER_LINEAR)\n",
    "    data1 = cv2.resize(data1, None, fx=resize_ratio, fy=resize_ratio, interpolation=cv2.INTER_LINEAR)\n",
    "    data2 = cv2.resize(data2, None, fx=resize_ratio, fy=resize_ratio, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # Blur data\n",
    "    data = cv2.filter2D(data,-1,kernel)\n",
    "    data = cv2.filter2D(data,-1,kernel)\n",
    "    data1 = cv2.filter2D(data1,-1,kernel)\n",
    "    data1 = cv2.filter2D(data1,-1,kernel)\n",
    "    data2 = cv2.filter2D(data2,-1,kernelp)\n",
    "    data2 = cv2.filter2D(data2,-1,kernelp)\n",
    "\n",
    "    hd, wd = data.shape\n",
    "\n",
    "    # ------------------------\n",
    "    # Create Semantic Map\n",
    "    # ------------------------\n",
    "\n",
    "    image = cv2.imread(png_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (wd, hd), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    map = np.zeros((hd, wd, chans))\n",
    "    colors = []\n",
    "\n",
    "    with open(labelmap_path, 'r') as f:\n",
    "        labelmap = [line.strip().split(':') for line in f.readlines()[2:]]\n",
    "\n",
    "    for i, label in enumerate(labelmap):\n",
    "        label_color = np.array(label[1].split(','), dtype=int)\n",
    "        colors.append(label_color)\n",
    "        map[:, :, i] = 255 - cv2.inRange(image, label_color - 10, label_color + 10)\n",
    "\n",
    "    # Load and process shade map if available\n",
    "    if os.path.isfile(f'/home/placido.falqueto/IRI_Barcelona/maps/13semantics/{map_name}/{map_name}_shades.png'):\n",
    "        png_path = '/home/placido.falqueto/IRI_Barcelona/maps/13semantics/'+map_name+'/'+map_name+'_shades.png'\n",
    "        # Load the PNG image\n",
    "        image = cv2.imread(png_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (wd,hd), interpolation = cv2.INTER_LINEAR)\n",
    "        map[:,:,-1] = 255-cv2.inRange(image, label_color-10, label_color+10)\n",
    "\n",
    "    map = map / 255 # Normalize map channels\n",
    "\n",
    "    # print(map.shape)\n",
    "    h, w, _ = map.shape\n",
    "\n",
    "    sem_map = np.zeros((h,w,3))\n",
    "    for i in range(chans):\n",
    "        sem = np.full((h,w,3),colors[i])\n",
    "        sem_map = np.stack((1-map[:,:,i],1-map[:,:,i],1-map[:,:,i]), axis=2)*sem+sem_map\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(sem_map/255)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # ------------------------\n",
    "    # Random Crop Generation\n",
    "    # ------------------------\n",
    "\n",
    "    selections = np.zeros((hd, wd))  \n",
    "    crops = np.zeros((n_crops, 2), dtype=int)\n",
    "\n",
    "    for i in range(n_crops):\n",
    "        while True:\n",
    "            aux_x = int(random.random() * (wd - div + 1))\n",
    "            aux_y = int(random.random() * (hd - div + 1))\n",
    "\n",
    "            submap = map[aux_y:aux_y + div, aux_x:aux_x + div, :]\n",
    "            subdata = data[aux_y:aux_y + div, aux_x:aux_x + div]\n",
    "            subdata1 = data1[aux_y:aux_y + div, aux_x:aux_x + div]\n",
    "            subdata2 = data2[aux_y:aux_y + div, aux_x:aux_x + div]\n",
    "\n",
    "            # Check if the crop contains enough information\n",
    "            if np.mean(submap) > 0.2 and np.mean(submap) < 1 and np.mean(subdata) > 0 and submap.shape == (div, div, chans):\n",
    "                crops[i, :] = (aux_x, aux_y)\n",
    "                selections += cv2.rectangle(np.zeros_like(selections), (crops[i, 0], crops[i, 1]), (crops[i, 0] + div - 1, crops[i, 1] + div - 1), (1, 0, 0), -1)\n",
    "                train_x = np.append(train_x, np.expand_dims(submap, axis=0), axis=0)\n",
    "                train_y = np.append(train_y, np.expand_dims(subdata, axis=0), axis=0)\n",
    "                train_y1 = np.append(train_y1, np.expand_dims(subdata1, axis=0), axis=0)\n",
    "                train_y2 = np.append(train_y2, np.expand_dims(subdata2, axis=0), axis=0)\n",
    "                break  \n",
    "\n",
    "    # Calculate weights with protection against division by zero\n",
    "    epsilon = 1e-6  # A small value to avoid division by zero\n",
    "    selections = 1.0 / (selections + epsilon)\n",
    "\n",
    "    data_pred = np.zeros((int(math.ceil(hd)),int(math.ceil(wd))))\n",
    "    data_pred1 = np.zeros((int(math.ceil(hd)),int(math.ceil(wd))))\n",
    "    data_pred2 = np.zeros((int(math.ceil(hd)),int(math.ceil(wd))))\n",
    "    for i in range(n_crops):\n",
    "        submap = map[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div, :]\n",
    "        subdata = data[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "        subdata1 = data1[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "        subdata2 = data2[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "        data_pred[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div] += subdata*selections[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "        data_pred1[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div] += subdata1*selections[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "        data_pred2[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div] += subdata2*selections[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(data_pred)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(data_pred1)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(data_pred2)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # ------------------------\n",
    "    # Data Augmentation\n",
    "    # ------------------------\n",
    "\n",
    "    for i in range(len(crops)): \n",
    "        for j in range(3):  # Rotations\n",
    "            train_x = np.append(train_x, np.expand_dims(np.rot90(train_x[i,:,:], k=j+1, axes=(0, 1)),axis=0), axis=0)\n",
    "            train_y = np.append(train_y, np.expand_dims(np.rot90(train_y[i,:,:], k=j+1, axes=(0, 1)),axis=0), axis=0)\n",
    "            train_y1 = np.append(train_y1, np.expand_dims(np.rot90(train_y1[i,:,:], k=j+1, axes=(0, 1)),axis=0), axis=0)\n",
    "            train_y2 = np.append(train_y2, np.expand_dims(np.rot90(train_y2[i,:,:], k=j+1, axes=(0, 1)),axis=0), axis=0)\n",
    "\n",
    "        # Flipping\n",
    "        train_x = np.append(train_x, [np.flip(submap, axis=0)], axis=0)\n",
    "        train_y = np.append(train_y, [np.flip(subdata, axis=0)], axis=0)\n",
    "        train_y1 = np.append(train_y1, [np.flip(subdata1, axis=0)], axis=0)\n",
    "        train_y2 = np.append(train_y2, [np.flip(subdata2, axis=0)], axis=0)\n",
    "\n",
    "        train_x = np.append(train_x, [np.flip(submap, axis=1)], axis=0)\n",
    "        train_y = np.append(train_y, [np.flip(subdata, axis=1)], axis=0)\n",
    "        train_y1 = np.append(train_y1, [np.flip(subdata1, axis=1)], axis=0)\n",
    "        train_y2 = np.append(train_y2, [np.flip(subdata2, axis=1)], axis=0)\n",
    "\n",
    "    # ------------------------\n",
    "    # Prepare Training Data\n",
    "    # ------------------------\n",
    "\n",
    "    train_x = np.delete(train_x, 0, 0)  # Remove placeholder row\n",
    "    train_y = np.delete(train_y, 0, 0)\n",
    "    train_y1 = np.delete(train_y1, 0, 0)\n",
    "    train_y2 = np.delete(train_y2, 0, 0)\n",
    "\n",
    "    print(f'Final shapes: train_x: {train_x.shape}, train_y: {train_y.shape}')\n",
    "\n",
    "    # ------------------------\n",
    "    # Save Training Data\n",
    "    # ------------------------\n",
    "\n",
    "    output_dir = f'{train_data_dir}/{div}crop_size/{chans}labels/{red}red/{map_name}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    np.savetxt(os.path.join(output_dir, 'train_X.csv'), np.insert(train_x.flatten(), 0, train_x.shape), delimiter=',', fmt='%f')\n",
    "    np.savetxt(os.path.join(output_dir, 'train_Y.csv'), np.insert(train_y.flatten(), 0, train_y.shape), delimiter=',', fmt='%f')\n",
    "    np.savetxt(os.path.join(output_dir, 'train_Y1.csv'), np.insert(train_y1.flatten(), 0, train_y1.shape), delimiter=',', fmt='%f')\n",
    "    np.savetxt(os.path.join(output_dir, 'train_Y2.csv'), np.insert(train_y2.flatten(), 0, train_y2.shape), delimiter=',', fmt='%f')\n",
    "\n",
    "# ------------------------\n",
    "# Main Loop\n",
    "# ------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    map_list = [\n",
    "        'stanford_bookstore0', 'stanford_bookstore4', 'stanford_coupa0', 'stanford_coupa1',\n",
    "        'stanford_coupa2', 'stanford_coupa3', 'stanford_gates0', 'stanford_gates1',\n",
    "        'stanford_gates2', 'stanford_gates3', 'stanford_hyang2', 'stanford_hyang3',\n",
    "        'stanford_hyang4',  'stanford_hyang10', 'stanford_little3', 'stanford_nexus0',\n",
    "        'stanford_nexus1',  'stanford_deathCircle0'\n",
    "    ]\n",
    "\n",
    "    for map_name in map_list:\n",
    "        process_map(map_name)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
