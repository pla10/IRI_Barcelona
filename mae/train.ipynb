{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "import models_mae\n",
    "import util.misc as misc\n",
    "from engine_pretrain import train_one_epoch\n",
    "from main_ViT import main, get_args_parser, run_one_image, DATASET_PATH, SemanticMapDataset\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_size = torch.cuda.device_count()\n",
    "print('world_size = %d' % world_size)\n",
    "os.environ['MASTER_ADDR'] = 'localhost'\n",
    "os.environ['MASTER_PORT'] = '12355'\n",
    "os.environ['WORLD_SIZE'] = str(world_size)\n",
    "\n",
    "torch.multiprocessing.spawn(main, nprocs=world_size, args=(world_size,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = get_args_parser()\n",
    "# args = args.parse_args()\n",
    "\n",
    "# args.resume = '/home/placido.falqueto/IRI_Barcelona/mae/output_dir/checkpoint-99.pth'\n",
    "\n",
    "# device = torch.device(args.device)\n",
    "\n",
    "# # define the model\n",
    "# model = models_mae.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "# model_without_ddp = model\n",
    "\n",
    "# eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()\n",
    "\n",
    "# if args.lr is None:  # only base_lr is specified\n",
    "#     args.lr = args.blr * eff_batch_size / 256\n",
    "\n",
    "# # following timm: set wd as 0 for bias and norm layers\n",
    "# param_groups = optim_factory.add_weight_decay(model_without_ddp, args.weight_decay)\n",
    "# optimizer = torch.optim.AdamW(param_groups, lr=args.lr, betas=(0.9, 0.95))\n",
    "# loss_scaler = NativeScaler()\n",
    "\n",
    "# misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data_dirs = ['stanford_hyang3']\n",
    "# print(f'TEST MAP: {test_data_dirs}\\n')\n",
    "\n",
    "# dataset_test = SemanticMapDataset(data_dirs=test_data_dirs)\n",
    "# data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=dataset_test.__len__(), shuffle=False)\n",
    "# features, target = next(iter(data_loader_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 1137 # np.random.randint(0, dataset_test.__len__())\n",
    "# print(f'sample id: {idx}')\n",
    "# features_test = features[idx].to('cuda')\n",
    "# target_test = target[idx].to('cuda')\n",
    "# run_one_image(features_test, target_test, model)\n",
    "\n",
    "# features_test = features_test.unsqueeze(dim=0)\n",
    "# target_test = target_test.unsqueeze(dim=0)\n",
    "\n",
    "# # run MAE\n",
    "# test_loss, pred_test, _ = model(features_test, target_test, mask_ratio=args.mask_ratio)\n",
    "# print(f'test_loss: {test_loss}')\n",
    "\n",
    "# N, C, H, W = pred_test.shape\n",
    "# pred_test = pred_test.reshape(N, C*H*W)\n",
    "# pred_test = torch.nn.functional.softmax(pred_test, dim=1)\n",
    "# pred_test = pred_test.reshape(N, C, H, W)\n",
    "# pred_test = pred_test.squeeze(dim=0)\n",
    "\n",
    "# N, H, W = target_test.shape\n",
    "# target_test = target_test.reshape(N, H*W)\n",
    "# target_test = torch.nn.functional.softmax(target_test, dim=1)\n",
    "# target_test = target_test.reshape(N, H, W)\n",
    "\n",
    "# print(f'KL divergence torch {torch.nn.functional.kl_div(pred_test.log(), target_test, reduction=\"batchmean\").item()}')\n",
    "# del features_test, target_test, pred_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
