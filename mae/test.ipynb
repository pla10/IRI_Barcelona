{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import skimage.measure\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "import models_mae\n",
    "import util.misc as misc\n",
    "from engine_pretrain import train_one_epoch\n",
    "from main_ViT import main, get_args_parser, run_one_image, DATASET_PATH, SemanticMapDataset\n",
    "import timm.optim.optim_factory as optim_factory\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINED MODELS\n",
    "## Variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = 64\n",
    "step = int(32/4)\n",
    "red = 1\n",
    "\n",
    "mask_ratio = 0.75\n",
    "test_name = 'stops1'\n",
    "n_crops = 200\n",
    "\n",
    "resize_ratio = 1\n",
    "\n",
    "box = 4\n",
    "kernel = np.ones((box,box),np.float32)/(box**2)\n",
    "\n",
    "map_list_t = ['master_big','master_big_closed','master_big_semiclosed']\n",
    "map_root_name = 'master_big'\n",
    "\n",
    "sem_dict = ['bicycle_road', 'building', 'entrance', 'grass', 'obstacle', 'parking', 'pedestrian_road', 'tree', 'vehicle_road', 'sitting_area', 'stairs', 'intersection_zone', 'shaded_area']\n",
    "chans = len(sem_dict)\n",
    "\n",
    "lut_in = [0, 10, 30, 50, 70, 110, 150, 255]\n",
    "lut_out = [0, 120, 200, 225, 240, 248, 250, 255]\n",
    "lut_in = [0, 10, 70, 150, 255]\n",
    "lut_out = [0, 130, 210, 245, 255]\n",
    "# plt.plot(lut_in, lut_out)\n",
    "lut_8u = np.interp(np.arange(0, 256), lut_in, lut_out).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = get_args_parser()\n",
    "# args = args.parse_args()\n",
    "\n",
    "# args.resume = '/home/placido.falqueto/IRI_Barcelona/mae/output_dir/test3-checkpoint-100.pth'\n",
    "\n",
    "# device = torch.device(args.device)\n",
    "\n",
    "# # define the model\n",
    "# model = models_mae.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)\n",
    "\n",
    "# model.to(device)\n",
    "\n",
    "# model_without_ddp = model\n",
    "\n",
    "# eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()\n",
    "\n",
    "# if args.lr is None:  # only base_lr is specified\n",
    "#     args.lr = args.blr * eff_batch_size / 256\n",
    "\n",
    "# # following timm: set wd as 0 for bias and norm layers\n",
    "# param_groups = optim_factory.add_weight_decay(model_without_ddp, args.weight_decay)\n",
    "# optimizer = torch.optim.AdamW(param_groups, lr=args.lr, betas=(0.9, 0.95))\n",
    "# loss_scaler = NativeScaler()\n",
    "\n",
    "# misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing closed corridors (completely or partially closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# red1 = 8\n",
    "\n",
    "# for map_name in map_list_t:\n",
    "#   print(map_name)\n",
    "\n",
    "#   lines = 0\n",
    "#   try:\n",
    "#     with open('/home/placido.falqueto/IRI_Barcelona/maps/13semantics/'+map_root_name+'/'+map_name+'-reduced.csv') as f:\n",
    "#       print('Reduced map found')\n",
    "#       lines = f.readlines()\n",
    "      \n",
    "#   except FileNotFoundError:\n",
    "#     with open('/home/placido.falqueto/IRI_Barcelona/maps/13semantics/'+map_root_name+'/'+map_name+'.csv') as f:\n",
    "#       print('Using original map')\n",
    "#       lines = f.readlines()\n",
    "\n",
    "#   h = len(lines)\n",
    "#   w = len(lines[0].split(','))\n",
    "\n",
    "#   # Converts data to a list of integers\n",
    "#   map = []\n",
    "#   for line in lines:\n",
    "#     map.extend([int(c) for c in line.split(',')])\n",
    "\n",
    "#   for lab_class in sem_dict:\n",
    "#     lines = 0\n",
    "#     try:\n",
    "#       with open('/home/placido.falqueto/IRI_Barcelona/maps/13semantics/'+map_name+'/'+map_name+'_sem_'+lab_class+'-reduced.csv') as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "#       hh = len(lines)\n",
    "#       ww = len(lines[0].split(','))\n",
    "\n",
    "#       if hh != h or ww != w:\n",
    "#         print(f'h: {h}\\tw: {w}')\n",
    "#         print(f'h: {hh}\\tw: {ww}')\n",
    "#         raise SystemExit(\"ERROR: Different sizes!!\")\n",
    "\n",
    "#       # Converts data to a list of integers\n",
    "#       for line in lines:\n",
    "#         map.extend([int(c) for c in line.split(',')])\n",
    "    \n",
    "#     except FileNotFoundError:\n",
    "#       try:\n",
    "#         with open('/home/placido.falqueto/IRI_Barcelona/maps/13semantics/'+map_root_name+'/'+map_name+'_sem_'+lab_class+'.csv') as f:\n",
    "#           lines = f.readlines()\n",
    "\n",
    "#         hh = len(lines)\n",
    "#         ww = len(lines[0].split(','))\n",
    "\n",
    "#         if hh != h or ww != w:\n",
    "#           print(f'h: {h}\\tw: {w}')\n",
    "#           print(f'h: {hh}\\tw: {ww}')\n",
    "#           raise SystemExit(\"ERROR: Different sizes!!\")\n",
    "\n",
    "#         # Converts data to a list of integers\n",
    "#         for line in lines:\n",
    "#           map.extend([int(c) for c in line.split(',')])\n",
    "\n",
    "#       except FileNotFoundError:\n",
    "#         for i in range(h):\n",
    "#           for j in range(w):\n",
    "#             map.extend([255])\n",
    "\n",
    "#   map = np.reshape(map,[chans,h,w])\n",
    "#   map = np.moveaxis(map, 0, -1)\n",
    "#   map = map/255\n",
    "\n",
    "#   map_aux = map\n",
    "#   map = np.zeros((int(math.ceil(h/red1)),int(math.ceil(w/red1)),chans))\n",
    "\n",
    "#   for idx in range(chans):\n",
    "#     map[:,:,idx] = skimage.measure.block_reduce(map_aux[:,:,idx], (red1,red1), np.max)\n",
    "\n",
    "#   # print(map.shape)\n",
    "#   h, w, _ = map.shape\n",
    "\n",
    "#   lines = 0\n",
    "#   with open('/home/placido.falqueto/IRI_Barcelona/maps/13semantics/'+map_root_name+'/humandensity-'+map_root_name+'-new.csv') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "#   hd = len(lines)\n",
    "#   wd = len(lines[0].split(','))\n",
    "\n",
    "#   # Converts data to a list of integers\n",
    "#   data = []\n",
    "#   for line in lines:\n",
    "#     data.extend([int(c) for c in line.split(',')])\n",
    "\n",
    "#   data = np.reshape(data,[hd,wd])\n",
    "  \n",
    "#   sigma = 10.0\n",
    "#   # data = skimage.filters.gaussian(data, sigma=(sigma, sigma), channel_axis=-1)\n",
    "#   data = skimage.measure.block_reduce(data, (red1,red1), np.max)\n",
    "#   # data = np.subtract(data, np.full((h, w), np.min(data)))/(np.max(data)-np.min(data))\n",
    "#   data = data*(map[:,:,0]>0)\n",
    "  \n",
    "#   lim_val = 0.15\n",
    "#   sigma = 0.5\n",
    "#   data_show = np.clip(data,0,np.max(data)*lim_val)/(np.max(data)*lim_val)\n",
    "#   data_show = cv2.LUT((data_show*255).astype(np.uint8), lut_8u).astype(np.float32)/255\n",
    "#   data_show = skimage.filters.gaussian(data_show, sigma=(sigma, sigma), channel_axis=-1)\n",
    "#   data = data_show/np.sum(data_show)\n",
    "\n",
    "#   # -----------------------------------------------------------------------------------\n",
    "#   # Random sample 32x32 windows in map\n",
    "#   n_crops = 100\n",
    "#   inserted = 0\n",
    "#   crops = np.zeros((n_crops,2),dtype=int)\n",
    "#   selections = np.zeros((h,w))\n",
    "#   for i in range(n_crops):\n",
    "#     background = np.zeros((h,w))\n",
    "#     flag = True\n",
    "#     while flag:\n",
    "#       aux_x = int(random.random()*(w-div+1))\n",
    "#       aux_y = int(random.random()*(h-div+1))\n",
    "#       flag = False\n",
    "#       # print(f'INSERTED: {inserted}  ->  x: {aux_x} and y: {aux_y}')\n",
    "#       # for z in range(inserted):\n",
    "#       #   if abs(aux_x-crops[z,0])<int(div/10) and abs(aux_y-crops[z,1])<int(div/10):\n",
    "#       #     # print('too close')\n",
    "#       #     flag = True\n",
    "#     crops[i,0] = aux_x\n",
    "#     crops[i,1] = aux_y\n",
    "#     selections = selections+cv2.rectangle(background,(crops[i,0],crops[i,1]),(crops[i,0]+div-1,crops[i,1]+div-1),(1,1,1),-1)\n",
    "#     inserted = inserted + 1\n",
    "#   selections = 1/selections\n",
    "\n",
    "#   # -----------------------------------------------------------------------------------\n",
    "#   # creating subplot and figure\n",
    "#   ax = plt.subplot(111)\n",
    "#   data_pred = np.zeros((int(math.ceil(h)),int(math.ceil(w))))\n",
    "#   kls = []\n",
    "#   for i in range(n_crops):\n",
    "#     submap = map[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div,:]\n",
    "#     data_aux = data[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "#     submap = torch.tensor(np.expand_dims(submap,axis=0), dtype=torch.float32).to(device)\n",
    "#     _, subdata, _ = model(submap)\n",
    "#     subdata = torch.einsum('nchw->nhwc', subdata).detach().cpu()\n",
    "#     subdata = subdata.squeeze(dim=0).squeeze(dim=2).numpy()\n",
    "#     data_pred[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div] += subdata*selections[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "    \n",
    "#   data = torch.Tensor(data)\n",
    "#   data_pred = torch.Tensor(data_pred)\n",
    "#   data = data/data.max()\n",
    "#   data_pred = data_pred/data_pred.max()\n",
    "  \n",
    "#   data_pred = data_pred/np.max(data_pred.detach().cpu().numpy())\n",
    "#   plt.imshow(np.multiply(np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2), np.stack((np.full(data_pred.shape,1),1-data_pred,1-data_pred),axis=2)), vmin=0, vmax=1)\n",
    "#   # plt.imshow(np.multiply(np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2), np.stack((np.full(data_pred.shape,1),1-data_pred,1-data_pred),axis=2)), vmin=0, vmax=1)\n",
    "#   ax.get_xaxis().set_visible(False)\n",
    "#   ax.get_yaxis().set_visible(False)\n",
    "#   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(features, data, title=''):\n",
    "    # features is [H, W, C]\n",
    "    # plt.imshow(torch.clip((features * imagenet_std + imagenet_mean) * 255, 0, 255).int())\n",
    "    data = data / data.max()\n",
    "    test11 = np.stack((features[:,:,0],features[:,:,0],features[:,:,0]),axis=2)\n",
    "    test11 = np.multiply(test11,np.stack((np.full(data.shape,1),1-data,1-data),axis=2))\n",
    "    for i in range(10):\n",
    "        alp = 0.5\n",
    "        test11 = np.multiply(test11,np.stack((features[:,:,i+1],features[:,:,i+1],features[:,:,i+1]),axis=2)*alp+(1-alp))\n",
    "    plt.imshow(test11)\n",
    "    plt.title(title, fontsize=23)\n",
    "    plt.axis('off')\n",
    "    return\n",
    "\n",
    "def run_one_image(x, target, model):\n",
    "    # run MAE\n",
    "    x = x.to('cuda')\n",
    "    target = target.to('cuda')\n",
    "    _, y, mask = model(x, mask_ratio=mask_ratio)\n",
    "    x = x.detach().cpu()\n",
    "    target = target.detach().cpu()\n",
    "    \n",
    "    y = y.squeeze(3).detach().cpu()\n",
    "\n",
    "    is_mae = 0\n",
    "    # if mask is not None:\n",
    "    #     # visualize the mask\n",
    "    #     mask = mask.detach()\n",
    "    #     mask = mask.unsqueeze(-1).repeat(1, 1, model.patch_embed.patch_size[0]**2 *1)  # (N, H*W, p*p*3)\n",
    "    #     mask = model.unpatchify(mask)  # 1 is removing, 0 is keeping\n",
    "    #     mask = torch.einsum('nchw->nhwc', mask).detach().cpu()\n",
    "    #     mask = mask.squeeze(-1)\n",
    "\n",
    "    #     plt.figure()\n",
    "    #     plt.imshow(mask[0])\n",
    "    #     plt.show()\n",
    "\n",
    "    #     # masked image\n",
    "    #     im_masked = target * (1 - mask) + mask*target.max()\n",
    "\n",
    "    #     is_mae = 1\n",
    "\n",
    "    # # MAE reconstruction pasted with visible patches\n",
    "    # im_paste = x * (1 - mask) + y * mask\n",
    "\n",
    "    # make the plt figure larger\n",
    "    plt.rcParams['figure.figsize'] = [24, 12] # W: 24, H: 12\n",
    "\n",
    "    plt.subplot(1, 2+is_mae, 1)\n",
    "    show_image(x[0], target[0], \"original\")\n",
    "\n",
    "    if is_mae:\n",
    "        plt.subplot(1, 2+is_mae, 2)\n",
    "        show_image(x[0], im_masked[0], \"masked\")\n",
    "\n",
    "    plt.subplot(1, 2+is_mae, 2+is_mae)\n",
    "    show_image(x[0], y[0], \"prediction\")\n",
    "\n",
    "    # plt.subplot(1, 4, 3)\n",
    "    # plt.imshow(target[0])\n",
    "    # plt.title('target', fontsize=23)\n",
    "    # plt.axis('off')\n",
    "\n",
    "    # plt.subplot(1, 4, 4)\n",
    "    # plt.imshow(y[0])\n",
    "    # plt.title('y', fontsize=23)\n",
    "    # plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kl1 = []\n",
    "kli1 = []\n",
    "was1 = []\n",
    "mse1 = []\n",
    "\n",
    "# get file list from /data/placido filtering only files that start with test_name\n",
    "map_list = [f for f in os.listdir('/data/placido') if f.startswith(test_name)]\n",
    "# remove the element in map_list that contains 'gates0'\n",
    "map_list = [f for f in map_list if 'gates0' not in f]\n",
    "\n",
    "\n",
    "for map_count, map_path in enumerate(map_list):\n",
    "  # map_path is test11-checkpoint-70-stanford_deathCircle0.pth, we need to isolate the map name (after stanford_ and before .pth)\n",
    "  map_name = map_path.split('-')[3].split('.')[0]\n",
    "  print(map_name)\n",
    "\n",
    "  args = get_args_parser()\n",
    "  args = args.parse_args()\n",
    "\n",
    "  args.resume = '/data/placido/'+map_path\n",
    "\n",
    "  device = torch.device(args.device)\n",
    "\n",
    "  # define the model\n",
    "  model = models_mae.__dict__[args.model](norm_pix_loss=args.norm_pix_loss)\n",
    "\n",
    "  model.to(device)\n",
    "\n",
    "  model_without_ddp = model\n",
    "\n",
    "  eff_batch_size = args.batch_size * args.accum_iter * misc.get_world_size()\n",
    "\n",
    "  if args.lr is None:  # only base_lr is specified\n",
    "      args.lr = args.blr * eff_batch_size / 256\n",
    "\n",
    "  # following timm: set wd as 0 for bias and norm layers\n",
    "  param_groups = optim_factory.add_weight_decay(model_without_ddp, args.weight_decay)\n",
    "  optimizer = torch.optim.AdamW(param_groups, lr=args.lr, betas=(0.9, 0.95))\n",
    "  loss_scaler = NativeScaler()\n",
    "\n",
    "  misc.load_model(args=args, model_without_ddp=model_without_ddp, optimizer=optimizer, loss_scaler=loss_scaler)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "  lines = 0\n",
    "  # with open('../maps/13semantics/'+map_name+'/humandensity-'+map_name+'-new.csv') as f:\n",
    "  # with open('../maps/13semantics/'+map_name+'/humandensity-'+map_name+'-vel.csv') as f:\n",
    "  with open('../maps/13semantics/'+map_name+'/humandensity-'+map_name+'-stop.csv') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "  hd = len(lines)\n",
    "  wd = len(lines[0].split(','))\n",
    "\n",
    "  # Converts data to a list of integers\n",
    "  data = []\n",
    "  for line in lines:\n",
    "    data.extend([float(c) for c in line.split(',')])\n",
    "\n",
    "  data = np.reshape(data,[hd,wd])\n",
    "  data = skimage.measure.block_reduce(data, (red,red), np.max)\n",
    "  data = cv2.resize(data, (int(wd*resize_ratio),int(hd*resize_ratio)), interpolation = cv2.INTER_LINEAR)\n",
    "\n",
    "\n",
    "  # print(data.shape)\n",
    "  hd, wd = data.shape\n",
    "  \n",
    "  data_show = data\n",
    "  data_show = cv2.filter2D(data_show,-1,kernel)\n",
    "  data_show = cv2.filter2D(data_show,-1,kernel)\n",
    "  data = data_show/np.sum(data_show)\n",
    "  data_show = data_show/np.max(data_show)\n",
    "\n",
    "  # -----------------------------------------------------------------------------------\n",
    "\n",
    "  png_path = '../maps/13semantics/'+map_name+'/'+map_name+'_colors.png'\n",
    "  labelmap_path = '../maps/13semantics/labelmap.txt'\n",
    "\n",
    "  # Read the labelmap file\n",
    "  with open(labelmap_path, 'r') as f:\n",
    "      labelmap = f.readlines()\n",
    "  labelmap.pop(0)\n",
    "  labelmap.pop(0)\n",
    "\n",
    "  # Load the PNG image\n",
    "  image = cv2.imread(png_path)\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "  image = cv2.resize(image, (wd,hd), interpolation = cv2.INTER_LINEAR)\n",
    "  map = np.zeros((hd,wd,chans))\n",
    "\n",
    "  # fig = plt.figure(figsize=(5,5))\n",
    "  # plt.imshow(image)\n",
    "  # plt.axis('off')\n",
    "  # plt.show()\n",
    "\n",
    "  colors = []\n",
    "  # Iterate over each label in the labelmap\n",
    "  for i, label in enumerate(labelmap):\n",
    "      label = label.strip().split(':')\n",
    "      label_name = label[0]\n",
    "      label_color = np.array(label[1].split(','), dtype=int)\n",
    "      colors.append(label_color)\n",
    "      # Create a mask for the pixels with the label color\n",
    "      map[:,:,i] = 255-cv2.inRange(image, label_color-10, label_color+10)\n",
    "\n",
    "  # check if map_name_shades.png exists and if so, add it to the map\n",
    "  if os.path.isfile('/home/placido.falqueto/IRI_Barcelona/maps/13semantics/'+map_name+'/'+map_name+'_shades.png'):\n",
    "    png_path = '/home/placido.falqueto/IRI_Barcelona/maps/13semantics/'+map_name+'/'+map_name+'_shades.png'\n",
    "    # Load the PNG image\n",
    "    image = cv2.imread(png_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (wd,hd), interpolation = cv2.INTER_LINEAR)\n",
    "    map[:,:,-1] = 255-cv2.inRange(image, label_color-10, label_color+10)\n",
    "\n",
    "  map = map/255\n",
    "\n",
    "  # print(map.shape)\n",
    "  h, w, _ = map.shape\n",
    "\n",
    "  sem_map = np.zeros((h,w,3))\n",
    "  for i in range(len(sem_dict)):\n",
    "    sem = np.full((h,w,3),colors[i])\n",
    "    sem_map = np.stack((1-map[:,:,i],1-map[:,:,i],1-map[:,:,i]), axis=2)*sem+sem_map\n",
    "\n",
    "  fig = plt.figure(figsize=(5,5))\n",
    "  plt.imshow(sem_map/255)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "  fig = plt.figure(figsize=(5,5))\n",
    "  plt.imshow(data_show)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "  # -----------------------------------------------------------------------------------\n",
    "  # Random sample 32x32 windows in map\n",
    "  inserted = 0\n",
    "  crops = np.zeros((n_crops,2),dtype=int)\n",
    "  selections = np.zeros((h,w))\n",
    "  for i in range(n_crops):\n",
    "    background = np.zeros((h,w))\n",
    "    flag = True\n",
    "    while flag:\n",
    "      aux_x = int(random.random()*(w-div+1))\n",
    "      aux_y = int(random.random()*(h-div+1))\n",
    "      flag = False\n",
    "      # print(f'INSERTED: {inserted}  ->  x: {aux_x} and y: {aux_y}')\n",
    "      # for z in range(inserted):\n",
    "      #   if abs(aux_x-crops[z,0])<int(div/10) and abs(aux_y-crops[z,1])<int(div/10):\n",
    "      #     # print('too close')\n",
    "      #     flag = True\n",
    "    crops[i,0] = aux_x\n",
    "    crops[i,1] = aux_y\n",
    "    selections = selections+cv2.rectangle(background,(crops[i,0],crops[i,1]),(crops[i,0]+div-1,crops[i,1]+div-1),(1,0,0),-1)\n",
    "    inserted = inserted + 1\n",
    "  selections = 1/selections\n",
    "\n",
    "  # -----------------------------------------------------------------------------------\n",
    "  # creating subplot and figure\n",
    "  # ax = plt.subplot(111)\n",
    "  data_pred = np.zeros((int(math.ceil(h)),int(math.ceil(w))))\n",
    "  kls = []\n",
    "  print(f'map shape: {map.shape}')\n",
    "  for i in range(n_crops):\n",
    "    submap = map[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div,:]\n",
    "    data_aux = data[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "    submap = torch.tensor(np.expand_dims(submap,axis=0), dtype=torch.float32).to(device)\n",
    "    data_aux = torch.tensor(np.expand_dims(data_aux,axis=0), dtype=torch.float32).to(device)\n",
    "    if submap.shape[1] != div or submap.shape[2] != div:\n",
    "      print('ERROR: wrong shape')\n",
    "      continue\n",
    "    _, subdata, _ = model(submap, mask_ratio=mask_ratio)\n",
    "    subdata = subdata.detach().cpu() #torch.einsum('nchw->nhwc', subdata).detach().cpu()\n",
    "    subdata = subdata.squeeze(dim=0).squeeze(dim=2).numpy()\n",
    "    data_pred[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div] += subdata*selections[crops[i,1]:crops[i,1]+div, crops[i,0]:crops[i,0]+div]\n",
    "    # Calculate the KL-divergence\n",
    "    \n",
    "  data = torch.Tensor(data)\n",
    "  data_pred = torch.Tensor(data_pred)\n",
    "  data = data/data.sum()\n",
    "  data_pred = data_pred/data_pred.sum()\n",
    "  data_pred[data_pred == 0] = 1e-12\n",
    "  data_pred[data_pred == 1] = 1-1e-12\n",
    "  data[data == 0] = 1e-12\n",
    "  data[data == 1] = 1-1e-12\n",
    "\n",
    "  kl = torch.nn.functional.kl_div(data_pred.unsqueeze(0).log(), data.unsqueeze(0), reduction='batchmean').item()\n",
    "  kli = torch.nn.functional.kl_div(data.unsqueeze(0).log(), data_pred.unsqueeze(0), reduction='batchmean').item()\n",
    "  h, w = data.shape\n",
    "  data_aux = (data / data.max() * 255).detach().cpu().numpy().astype(int)\n",
    "  pred_aux = (data_pred / data_pred.max() * 255).detach().cpu().numpy().astype(int)\n",
    "  print(f'data_aux max: {np.max(data_aux)}')\n",
    "  print(f'pred_aux max: {np.max(pred_aux)}')\n",
    "  hist_a = [0.0] * 256\n",
    "  for i in range(h):\n",
    "    for j in range(w):\n",
    "      hist_a[data_aux[i, j]] += 1\n",
    "  hist_b = [0.0] * 256\n",
    "  for i in range(h):\n",
    "    for j in range(w):\n",
    "      hist_b[pred_aux[i, j]] += 1\n",
    "  was = wasserstein_distance(hist_a, hist_b)\n",
    "  mse = ((data - data_pred)**2).mean()\n",
    "\n",
    "  data = data/data.max()\n",
    "  data_pred = data_pred/data_pred.max()\n",
    "  plt.figure(figsize=(4,4))\n",
    "  plt.imshow(np.multiply(np.stack((1-data,1-data,np.full(data.shape,1)),axis=2), np.stack((np.full(data_pred.shape,1),1-data_pred,1-data_pred),axis=2)))\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "  \n",
    "  data_pred = data_pred/np.max(data_pred.detach().cpu().numpy())\n",
    "  plt.figure(figsize=(5,5))\n",
    "  plt.imshow(data_pred)\n",
    "  # plt.imshow(np.multiply(np.stack((map[:,:,0],map[:,:,0],map[:,:,0]),axis=2), np.stack((np.full(data_pred.shape,1),1-data_pred,1-data_pred),axis=2)), vmin=0, vmax=1)\n",
    "  plt.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "  print(f'KL-divergence: {kl}')\n",
    "  print(f'Reverse KL-divergence: {kli}')\n",
    "  print(f'EMD: {was:.2f}')\n",
    "  print(f'MSE: {mse:.2e}')\n",
    "  kl1 = np.append(kl1,kl)\n",
    "  kli1 = np.append(kli1,kli)\n",
    "  was1 = np.append(was1,was)\n",
    "  mse1 = np.append(mse1,mse)\n",
    "  # import sys\n",
    "  # sys.exit(0)\n",
    "\n",
    "print('-----------------------------')\n",
    "print(f'Mean KL-divergence: {np.mean(kl1)}')\n",
    "print(f'Std KL-divergence: {np.std(kl1)}')\n",
    "print(f'Mean reverse KL-divergence: {np.mean(kli1)}')\n",
    "print(f'Std reverse KL-divergence: {np.std(kli1)}')\n",
    "print(f'Mean EMD: {np.mean(was1):.2f}')\n",
    "print(f'Std EMD: {np.std(was1):.2f}')\n",
    "\n",
    "# plot KL-div\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(kl1, label='KL-divergence')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot all metrics normalized\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(kl1 / np.max(kl1), label='KL-divergence')\n",
    "plt.plot(kli1 / np.max(kli1), label='Reverse KL-divergence')\n",
    "plt.plot(was1 / np.max(was1), label='EMD')\n",
    "plt.plot(mse1 / np.max(mse1), label='MSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
