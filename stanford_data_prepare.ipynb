{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import skimage.measure\n",
    "\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 12\n",
    "\n",
    "div = 32\n",
    "step = int(32/4)\n",
    "red = [4, 4, 2, 1, 2, 4, 6, 4, 8, 8, 4, 6, 4, 6, 6] # Check the estimated_scales.yaml -> the resolution has to be 0.4 meters/pixel\n",
    "# coupa0 e 3   ->     red = 15\n",
    "# gates2       ->     red = 11 o 10 (10)\n",
    "# hyang1       ->     red = 5\n",
    "# hyang10      ->     red = 8\n",
    "red = red[idx]\n",
    "location_list = ['coupa', 'coupa', 'gates', 'hyang', 'hyang', 'nexus', 'little', 'nexus', 'coupa', 'coupa', 'hyang', 'hyang', 'hyang', 'hyang', 'hyang']\n",
    "number_list = ['0', '3', '2', '1', '10', '0', '3', '1', '1', '2', '0', '2', '3', '4', '5']\n",
    "\n",
    "sem_dict = ['cash', 'entrance', 'light', 'sit', 'stairs', 'trash', 'tree','restricted','grass','intersection']\n",
    "chans = len(sem_dict)+1\n",
    "\n",
    "lut_in = [0, 20, 50, 100, 150, 255]\n",
    "lut_out = [0, 100, 180, 220, 240, 255]\n",
    "lut_8u = np.interp(np.arange(0, 256), lut_in, lut_out).astype(np.uint8)\n",
    "\n",
    "location = location_list[idx]\n",
    "number = number_list[idx]\n",
    "map_name = 'stanford_'+location+number\n",
    "directory_dataset = '/'+location+'/video'+number+'/'\n",
    "print(map_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.load_model('IRI_models/4set_8px_steps_only_paths')\n",
    "\n",
    "# model1 = tf.keras.models.load_model('IRI_models/4set_8px_steps_only_vels')\n",
    "\n",
    "# model2 = tf.keras.models.load_model('IRI_models/4set_8px_steps_only_stops')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to check if the red variable is correct to scale to the correct homography. Check on google maps and note that each pixel should be 0.4 meters (32 pixels = 12.8m)\n",
    "\n",
    "cap = cv2.VideoCapture('Stanford_Drone_Dataset/video'+directory_dataset+'video.mp4')\n",
    "df = pd.read_csv('Stanford_Drone_Dataset/annotations'+directory_dataset+'annotations.txt', delimiter=' ', header=None)\n",
    " \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    "ret, frame = cap.read()\n",
    "print(frame.shape)\n",
    "\n",
    "frame = skimage.measure.block_reduce(frame, (red,red,1), np.max)\n",
    "\n",
    "plt.figure(figsize=(3,3))\n",
    "im = plt.imshow(frame)\n",
    "ax = plt.gca()\n",
    "# ax.set_xticks(np.arange(0, 170, 5))\n",
    "# ax.set_yticks(np.arange(0, 90, 5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('Stanford_Drone_Dataset/video'+directory_dataset+'video.mp4')\n",
    "df = pd.read_csv('Stanford_Drone_Dataset/annotations'+directory_dataset+'annotations.txt', delimiter=' ', header=None)\n",
    " \n",
    "# Check if camera opened successfully\n",
    "if (cap.isOpened()== False): \n",
    "  print(\"Error opening video stream or file\")\n",
    "ret, frame = cap.read()\n",
    "cv2.imwrite('maps/semantics/'+map_name+'/reference.png', frame)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "ref_frame = frame\n",
    "\n",
    "h = frame.shape[0]\n",
    "w = frame.shape[1]\n",
    "\n",
    "dt = 1/cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "data = np.zeros((h,w))\n",
    "data_vel = np.zeros((h,w))\n",
    "data_stops = np.zeros((h,w))\n",
    "pos = np.full(len(df[0].unique()), {'x': 0.0, 'y': 0.0})\n",
    "vels = np.full(len(df[0].unique()), {'x': 0.0, 'y': 0.0})\n",
    "\n",
    "ratioy = (df[4].max())/h\n",
    "ratiox = (df[3].max())/w\n",
    "\n",
    "frame_count = 0\n",
    "# Read until video is completed\n",
    "while(cap.isOpened()):\n",
    "  # Capture frame-by-frame\n",
    "  ret, frame = cap.read()\n",
    "  if ret == True:\n",
    "    if frame_count % 1 == 0:\n",
    " \n",
    "      ce = df[df[5]==frame_count] # current elements in frame (people and bikes)\n",
    "      ce = ce[ce[9]=='Pedestrian']# filter only people\n",
    "\n",
    "      vel_temp = np.zeros((h,w))\n",
    "      for i in ce.index:\n",
    "        el = ce.loc[i]\n",
    "\n",
    "        # color = (255, 0, 0)\n",
    "        # hcolor = (255, 255, 0)\n",
    "        # radius = 10\n",
    "        # thickness = -1\n",
    "        \n",
    "        alpha = 0.2\n",
    "\n",
    "        centerOfCircle = (int((el[2]+el[4])/2/ratioy),int((el[1]+el[3])/2/ratiox))\n",
    "        if(pos[el[0]]['x']==0 and pos[el[0]]['y']==0):\n",
    "          pos[el[0]] = {'x': centerOfCircle[0],'y': centerOfCircle[1]}\n",
    "        vels[el[0]] = {'x': alpha*0.015*(centerOfCircle[0]-pos[el[0]]['x'])/dt + (1-alpha)*vels[el[0]]['x'],'y': alpha*0.015*(centerOfCircle[1]-pos[el[0]]['y'])/dt + (1-alpha)*vels[el[0]]['y']}\n",
    "        pos[el[0]] = {'x': centerOfCircle[0],'y': centerOfCircle[1]}\n",
    "\n",
    "        if centerOfCircle[0] < h and centerOfCircle[1] < w and el[6] == 0 or el[7] == 1:\n",
    "          data[centerOfCircle] = data[centerOfCircle] + 1\n",
    "          vel_temp[centerOfCircle] = math.sqrt(vels[el[0]]['x']**2+vels[el[0]]['y']**2)\n",
    "          if(vel_temp[centerOfCircle] < 0.1):\n",
    "            data_stops[centerOfCircle] = data_stops[centerOfCircle] + 1\n",
    "          #   frame = cv2.circle(frame, (centerOfCircle[1],centerOfCircle[0]), radius, hcolor, thickness)\n",
    "          # else:\n",
    "          #   frame = cv2.circle(frame, (centerOfCircle[1],centerOfCircle[0]), radius, color, thickness)\n",
    "          # frame = cv2.putText(frame, \"{:.3f}\".format(vel_temp[centerOfCircle]), (centerOfCircle[1],centerOfCircle[0]), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2, cv2.LINE_AA)\n",
    "\n",
    "      data_vel = data_vel * (vel_temp == 0) + data_vel * (vel_temp > 0)*0.5 + vel_temp * (data_vel > 0)*0.5 + vel_temp * (data_vel == 0)\n",
    "\n",
    "      # # Display the resulting frame\n",
    "      # clear_output(wait=True)\n",
    "      # plt.figure(figsize=(1956/100,1080/100))\n",
    "      # plt.imshow(frame)\n",
    "      # plt.show()\n",
    "    \n",
    "    frame_count = frame_count + 1\n",
    " \n",
    "  # Break the loop\n",
    "  else: \n",
    "    \n",
    "    break\n",
    " \n",
    "# When everything done, release the video capture object\n",
    "cap.release()\n",
    "\n",
    "record_data = data\n",
    "record_data_vel = data_vel\n",
    "record_data_stops = data_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = record_data\n",
    "data_vel = record_data_vel\n",
    "data_stops = record_data_stops\n",
    "\n",
    "data = np.clip(data,0,np.max(data)*0.2)/(np.max(data)*0.2)\n",
    "data = cv2.LUT((data/np.max(data)*255).astype(np.uint8), lut_8u).astype(float)/255\n",
    "data = skimage.measure.block_reduce(data, (red,red), np.max)\n",
    "ref_frame_red = skimage.measure.block_reduce(ref_frame, (red,red,1), np.max)/510+0.5\n",
    "data = np.subtract(data, np.full((data.shape[0], data.shape[1]), np.min(data)))/(np.max(data)-np.min(data))\n",
    "\n",
    "plt.figure(figsize=(w/200,h/200))\n",
    "# plt.imshow(np.multiply(np.stack((map_var[:,:,0],map_var[:,:,0],map_var[:,:,0]),axis=2), np.stack((np.full(data.shape,1),1-data,1-data),axis=2)))\n",
    "plt.imshow(np.multiply(ref_frame_red, np.stack((np.full(data.shape,1),1-data,1-data),axis=2)))\n",
    "plt.show()\n",
    "\n",
    "np.savetxt('maps/semantics/'+map_name+'/humandensity-'+map_name+'-new.csv', data*255, delimiter=',', fmt='%3d')\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "data_vel = skimage.measure.block_reduce(data_vel, (red,red), np.max)\n",
    "data_vel = np.subtract(data_vel, np.full((data.shape[0], data.shape[1]), np.min(data_vel)))/(np.max(data_vel)-np.min(data_vel))\n",
    "\n",
    "plt.figure(figsize=(w/200,h/200))\n",
    "# plt.imshow(np.multiply(np.stack((map_var[:,:,0],map_var[:,:,0],map_var[:,:,0]),axis=2), np.stack((np.full(data_vel.shape,1),1-data_vel,1-data_vel),axis=2)))\n",
    "plt.imshow(np.multiply(ref_frame_red, np.stack((np.full(data_vel.shape,1),1-data_vel,1-data_vel),axis=2)))\n",
    "plt.show()\n",
    "\n",
    "np.savetxt('maps/semantics/'+map_name+'/humandensity-'+map_name+'-vel.csv', data_vel*255, delimiter=',', fmt='%3d')\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "data_stops = skimage.measure.block_reduce(data_stops, (red,red), np.max)\n",
    "data_stops = np.subtract(data_stops, np.full((data.shape[0], data.shape[1]), np.min(data_stops)))/(np.max(data_stops)-np.min(data_stops))\n",
    "\n",
    "plt.figure(figsize=(w/200,h/200))\n",
    "# plt.imshow(np.multiply(np.stack((map_var[:,:,0],map_var[:,:,0],map_var[:,:,0]),axis=2), np.stack((np.full(data_stops.shape,1),1-data_stops,1-data_stops),axis=2)))\n",
    "plt.imshow(np.multiply(ref_frame_red, np.stack((np.full(data_stops.shape,1),1-data_stops,1-data_stops),axis=2)))\n",
    "plt.show()\n",
    "\n",
    "np.savetxt('maps/semantics/'+map_name+'/humandensity-'+map_name+'-stop.csv', data_stops*255, delimiter=',', fmt='%3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_name)\n",
    "\n",
    "lines = 0\n",
    "with open('maps/semantics/'+map_name+'/'+map_name+'.csv') as f:\n",
    "  lines = f.readlines()\n",
    "\n",
    "h = len(lines)\n",
    "w = len(lines[0].split(','))\n",
    "\n",
    "# Converts data to a list of integers\n",
    "map_var = []\n",
    "for line in lines:\n",
    "  map_var.extend([int(c) for c in line.split(',')])\n",
    "\n",
    "for lab_class in sem_dict:\n",
    "  lines = 0\n",
    "  try:\n",
    "    with open('maps/semantics/'+map_name+'/'+map_name+'_sem_'+lab_class+'.csv') as f:\n",
    "      lines = f.readlines()\n",
    "\n",
    "    hh = len(lines)\n",
    "    ww = len(lines[0].split(','))\n",
    "\n",
    "    if hh != h or ww != w:\n",
    "      print(f'h: {h}\\tw: {w}')\n",
    "      print(f'h: {hh}\\tw: {ww}')\n",
    "      raise SystemExit(\"ERROR: Different sizes!!\")\n",
    "\n",
    "    # Converts data to a list of integers\n",
    "    for line in lines:\n",
    "      map_var.extend([int(c) for c in line.split(',')])\n",
    "\n",
    "  except FileNotFoundError:\n",
    "    for i in range(h):\n",
    "      for j in range(w):\n",
    "        map_var.extend([255])\n",
    "\n",
    "map_var = np.reshape(map_var,[chans,h,w])\n",
    "map_var = np.moveaxis(map_var, 0, -1)\n",
    "map_var = map_var/255\n",
    "\n",
    "print(map_var.shape)\n",
    "\n",
    "map_aux = map_var\n",
    "map_var = np.zeros((int(math.ceil(h/red)),int(math.ceil(w/red)),chans))\n",
    "\n",
    "for idx in range(chans):\n",
    "  map_var[:,:,idx] = skimage.measure.block_reduce(map_aux[:,:,idx], (red,red), np.min)\n",
    "h, w, _ = map_var.shape\n",
    "\n",
    "print(map_var.shape)\n",
    "\n",
    "np.savetxt('maps/semantics/'+map_name+'/'+map_name+'-reduced.csv', map_var[:,:,0]*255, delimiter=',', fmt='%3d')\n",
    "for index, lab_class in enumerate(sem_dict):\n",
    "  np.savetxt('maps/semantics/'+map_name+'/'+map_name+'_sem_'+lab_class+'-reduced.csv', map_var[:,:,index+1]*255, delimiter=',', fmt='%3d')\n",
    "\n",
    "print(sem_dict)\n",
    "plt.figure(figsize=(20,5))\n",
    "for i in range(len(sem_dict)):\n",
    "  ax = plt.subplot(1, len(sem_dict), i+1)\n",
    "  alp = 0.5\n",
    "  ax.imshow(np.multiply(np.stack((map_var[:,:,0],map_var[:,:,0],map_var[:,:,0]),axis=2),np.stack((map_var[:,:,i+1],map_var[:,:,i+1],map_var[:,:,i+1]),axis=2)*alp+(1-alp)), vmin=0, vmax=1)\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
